{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059b90ff-00c5-40fb-870a-1c7d48253822",
   "metadata": {},
   "source": [
    "# Small Code Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1867432-2112-47a4-a925-1ec7c0188dde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Huggingface Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b28c3d-cef4-4d50-a8bd-3a7a59f3949e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# batch_size_cockatiel = 64\n",
    "\n",
    "# n_concepts = 20\n",
    "# #n_reviews = 20000\n",
    "# n_excerpts = 50000\n",
    "# n_reviews_for_excerpts = 100000\n",
    "\n",
    "# positive_class = 1\n",
    "# negative_class = 0\n",
    "\n",
    "# output_names = ['negative', 'positive']\n",
    "# parent_path = \"data/huggingface_test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324ca18-7a71-4911-b249-901f8831f4ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5ef05-44e4-4d06-8ad7-c4e98e427cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, samples, device):\n",
    "    \n",
    "    samples = [[sample[0], 1 if sample[1] == 'positive' else 0] for sample in samples]\n",
    "\n",
    "    r = np.array(list(map(lambda z: z[0], samples)))\n",
    "    tokenized_samples = tokenize(r, tokenizer, device)\n",
    "    preds = model(**tokenized_samples)\n",
    "\n",
    "    # Extract texts and raw prediction scores\n",
    "    raw_scores = preds.cpu().detach().numpy()\n",
    "\n",
    "    # Extract predicted labels\n",
    "    predictions = np.argmax(raw_scores, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    true_labels = [int(sample[1]) for sample in samples]\n",
    "    \n",
    "    return predictions, true_labels\n",
    "\n",
    "\n",
    "def evaluate(y_test, y_pred):\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", round(accuracy, 2))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    metrics = classification_report(y_test, y_pred)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e53394-8b8e-450e-b515-5fa8b2f14b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, samples, device):\n",
    "    \n",
    "    samples = [[sample[0], 1 if sample[1] == 'positive' else 0] for sample in samples]\n",
    "\n",
    "    r = np.array(list(map(lambda z: z[0], samples)))\n",
    "\n",
    "    tokenized_samples = tokenize(r, tokenizer, device)\n",
    "\n",
    "    preds = model(**tokenized_samples)\n",
    "\n",
    "    # Extract texts and raw prediction scores\n",
    "    texts = r\n",
    "    raw_scores = preds.cpu().detach().numpy()\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(torch.tensor(raw_scores), dim=1)\n",
    "\n",
    "    # positive_probabilities = probabilities[:, 1]  # Probability for the positive class\n",
    "    # negative_probabilities = probabilities[:, 0]  # Probability for the negative class\n",
    "\n",
    "    # Extract predicted labels\n",
    "    predictions = np.argmax(raw_scores, axis=1)\n",
    "    \n",
    "    # result = {\n",
    "    #     'Text': texts,\n",
    "    #     'Predictions': predictions,\n",
    "    #     'Positive Probability': positive_probabilities,\n",
    "    #     'Negative Probability': negative_probabilities,\n",
    "    # }\n",
    "\n",
    "    # Calculate accuracy\n",
    "    true_labels = [int(sample[1]) for sample in samples]\n",
    "    \n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b8cae-a64b-4922-b56f-88c8df6e9f83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764660b-203c-4e3f-9bd8-4368a92ae73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on some samples\n",
    "# y_pred, labels = batch_predict(model.forward, tokenizer, data_np[:1000], batch_size, device)\n",
    "\n",
    "# # Compute the activations on which to apply the NMF\n",
    "# features, labels = batch_predict(model.features, tokenizer, data_np[:1000], batch_size, device)\n",
    "\n",
    "# # Go from these activations to the final prediction\n",
    "# y_pred_bis = model.end_model(features)\n",
    "\n",
    "# print(\"\\nAccuracy for classic model        :\", torch.mean((torch.argmax(y_pred, -1) == labels.to(device)).float()))\n",
    "# print(\"Accuracy for model in 'two parts' :\", torch.mean((torch.argmax(y_pred_bis, -1) == labels.to(device)).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4b51557-bb0d-42be-bba0-923c3e7a66b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11926   299]\n",
      " [  546 12226]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97     12225\n",
      "         1.0       0.98      0.96      0.97     12772\n",
      "\n",
      "    accuracy                           0.97     24997\n",
      "   macro avg       0.97      0.97      0.97     24997\n",
      "weighted avg       0.97      0.97      0.97     24997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions, labels = predict(model, tokenizer, data_np[25000:], device)\n",
    "# evaluate(predictions, labels)\n",
    "# clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bfba0d1-ccad-4605-85e0-a97b3e01fb50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 909   33]\n",
      " [  38 1020]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       942\n",
      "         1.0       0.97      0.96      0.97      1058\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.96      0.96      0.96      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions, labels = predict(model, tokenizer, data_np[25000:27000], device)\n",
    "# evaluate(predictions, labels)\n",
    "# clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5539c-38c1-4b7f-bd79-ee1c4e2bb936",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sarcasm Dataset Control (excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b12ff183-187e-4dce-b79f-de86bc350696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a698e3620944ff198ae55713553c70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/400 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onurp\\anaconda3\\envs\\cockatiel\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\onurp\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1efceee29f440e88d2bee99622555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae422241858b4a579c132a19e7424d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c81b30325314964ae40f366be1e2f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb64b96171447369693b8c953d79e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# import string\n",
    "\n",
    "# def preprocess_data(text: str) -> str:\n",
    "#     return text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).strip()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"helinivan/english-sarcasm-detector\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"helinivan/english-sarcasm-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3eb5e25-84d7-4529-b3da-6c3fd033e06f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it sarcastic 0, Confidence: 0.7894307374954224\n",
      "Is it sarcastic 0, Confidence: 0.9768994450569153\n",
      "Is it sarcastic 0, Confidence: 0.8849927186965942\n",
      "Is it sarcastic 0, Confidence: 0.9927780032157898\n",
      "Is it sarcastic 0, Confidence: 0.9407753944396973\n",
      "Is it sarcastic 0, Confidence: 0.7536627650260925\n",
      "Is it sarcastic 0, Confidence: 0.993133544921875\n",
      "Is it sarcastic 0, Confidence: 0.9858521223068237\n",
      "Is it sarcastic 0, Confidence: 0.9944425225257874\n",
      "Is it sarcastic 0, Confidence: 0.8262344598770142\n",
      "Is it sarcastic 0, Confidence: 0.9960529804229736\n",
      "Is it sarcastic 0, Confidence: 0.988563060760498\n",
      "Is it sarcastic 0, Confidence: 0.9825398921966553\n",
      "Is it sarcastic 0, Confidence: 0.9499245285987854\n",
      "Is it sarcastic 0, Confidence: 0.99533611536026\n",
      "Is it sarcastic 0, Confidence: 0.9950424432754517\n",
      "Is it sarcastic 0, Confidence: 0.9919399619102478\n",
      "Is it sarcastic 0, Confidence: 0.9935635328292847\n",
      "Is it sarcastic 0, Confidence: 0.9713146090507507\n",
      "Is it sarcastic 0, Confidence: 0.9960530996322632\n",
      "Is it sarcastic 0, Confidence: 0.9871127009391785\n",
      "Is it sarcastic 0, Confidence: 0.9912478923797607\n",
      "Is it sarcastic 0, Confidence: 0.9893844723701477\n",
      "Is it sarcastic 0, Confidence: 0.9908857345581055\n",
      "Is it sarcastic 0, Confidence: 0.9921606183052063\n",
      "Is it sarcastic 0, Confidence: 0.9947925209999084\n",
      "Is it sarcastic 0, Confidence: 0.9805700182914734\n",
      "Is it sarcastic 0, Confidence: 0.9333847761154175\n",
      "Is it sarcastic 0, Confidence: 0.9928079843521118\n",
      "Is it sarcastic 0, Confidence: 0.9908943176269531\n",
      "Is it sarcastic 0, Confidence: 0.9950289130210876\n",
      "Is it sarcastic 0, Confidence: 0.9919173121452332\n",
      "Is it sarcastic 0, Confidence: 0.9789471626281738\n",
      "Is it sarcastic 0, Confidence: 0.9935762286186218\n",
      "Is it sarcastic 0, Confidence: 0.9817442297935486\n",
      "Is it sarcastic 0, Confidence: 0.9905063509941101\n",
      "Is it sarcastic 0, Confidence: 0.99588543176651\n",
      "Is it sarcastic 0, Confidence: 0.9954582452774048\n",
      "Is it sarcastic 0, Confidence: 0.8596563935279846\n",
      "Is it sarcastic 0, Confidence: 0.9900586009025574\n",
      "Is it sarcastic 0, Confidence: 0.9073140025138855\n",
      "Is it sarcastic 0, Confidence: 0.9482462406158447\n",
      "Is it sarcastic 0, Confidence: 0.6461066603660583\n",
      "Is it sarcastic 0, Confidence: 0.9445118308067322\n",
      "Is it sarcastic 0, Confidence: 0.7666977047920227\n",
      "Is it sarcastic 0, Confidence: 0.986343502998352\n",
      "Is it sarcastic 1, Confidence: 0.7043170928955078\n",
      "Is it sarcastic 0, Confidence: 0.8849042057991028\n",
      "Is it sarcastic 1, Confidence: 0.5197905898094177\n",
      "Is it sarcastic 0, Confidence: 0.7576889991760254\n",
      "Is it sarcastic 0, Confidence: 0.5080985426902771\n",
      "Is it sarcastic 0, Confidence: 0.9909080266952515\n",
      "Is it sarcastic 0, Confidence: 0.8598822355270386\n",
      "Is it sarcastic 0, Confidence: 0.9733874201774597\n",
      "Is it sarcastic 1, Confidence: 0.5430458188056946\n",
      "Is it sarcastic 1, Confidence: 0.5067456364631653\n",
      "Is it sarcastic 0, Confidence: 0.6745243668556213\n",
      "Is it sarcastic 0, Confidence: 0.7036610245704651\n",
      "Is it sarcastic 0, Confidence: 0.6304394006729126\n",
      "Is it sarcastic 0, Confidence: 0.837085485458374\n",
      "Is it sarcastic 0, Confidence: 0.9245682954788208\n",
      "Is it sarcastic 0, Confidence: 0.9685803651809692\n",
      "Is it sarcastic 0, Confidence: 0.9922000765800476\n",
      "Is it sarcastic 0, Confidence: 0.9450436234474182\n",
      "Is it sarcastic 0, Confidence: 0.9855247139930725\n",
      "Is it sarcastic 0, Confidence: 0.978821873664856\n",
      "Is it sarcastic 0, Confidence: 0.9799391031265259\n",
      "Is it sarcastic 0, Confidence: 0.98813396692276\n",
      "Is it sarcastic 0, Confidence: 0.9414555430412292\n",
      "Is it sarcastic 0, Confidence: 0.9953558444976807\n",
      "Is it sarcastic 0, Confidence: 0.8733803629875183\n",
      "Is it sarcastic 0, Confidence: 0.9938608407974243\n",
      "Is it sarcastic 0, Confidence: 0.983361542224884\n",
      "Is it sarcastic 0, Confidence: 0.9803480505943298\n",
      "Is it sarcastic 0, Confidence: 0.9944702386856079\n",
      "Is it sarcastic 0, Confidence: 0.9940391778945923\n",
      "Is it sarcastic 0, Confidence: 0.9914831519126892\n",
      "Is it sarcastic 0, Confidence: 0.9933171272277832\n",
      "Is it sarcastic 0, Confidence: 0.9830622673034668\n",
      "Is it sarcastic 0, Confidence: 0.9960916638374329\n",
      "Is it sarcastic 0, Confidence: 0.983335018157959\n",
      "Is it sarcastic 0, Confidence: 0.9931095838546753\n",
      "Is it sarcastic 0, Confidence: 0.9876052141189575\n",
      "Is it sarcastic 0, Confidence: 0.9916700124740601\n",
      "Is it sarcastic 0, Confidence: 0.9880173206329346\n",
      "Is it sarcastic 0, Confidence: 0.9949277639389038\n",
      "Is it sarcastic 0, Confidence: 0.9628873467445374\n",
      "Is it sarcastic 0, Confidence: 0.9229528903961182\n",
      "Is it sarcastic 0, Confidence: 0.9934490323066711\n",
      "Is it sarcastic 0, Confidence: 0.9838385581970215\n",
      "Is it sarcastic 0, Confidence: 0.9959945678710938\n",
      "Is it sarcastic 0, Confidence: 0.9894698858261108\n",
      "Is it sarcastic 0, Confidence: 0.9700068831443787\n",
      "Is it sarcastic 0, Confidence: 0.992642343044281\n",
      "Is it sarcastic 0, Confidence: 0.9858507513999939\n",
      "Is it sarcastic 0, Confidence: 0.9877591729164124\n",
      "Is it sarcastic 0, Confidence: 0.9947774410247803\n",
      "Is it sarcastic 0, Confidence: 0.995543897151947\n",
      "Is it sarcastic 0, Confidence: 0.8290159702301025\n",
      "Is it sarcastic 0, Confidence: 0.9888596534729004\n",
      "Is it sarcastic 0, Confidence: 0.9820098280906677\n",
      "Is it sarcastic 0, Confidence: 0.7942556142807007\n",
      "Is it sarcastic 0, Confidence: 0.9514062404632568\n",
      "Is it sarcastic 0, Confidence: 0.8897610902786255\n",
      "Is it sarcastic 0, Confidence: 0.9887027740478516\n",
      "Is it sarcastic 1, Confidence: 0.5327762365341187\n",
      "Is it sarcastic 0, Confidence: 0.9484598636627197\n",
      "Is it sarcastic 1, Confidence: 0.6856871843338013\n",
      "Is it sarcastic 0, Confidence: 0.9378963708877563\n",
      "Is it sarcastic 1, Confidence: 0.5577535629272461\n",
      "Is it sarcastic 0, Confidence: 0.9440584778785706\n",
      "Is it sarcastic 1, Confidence: 0.5827739834785461\n",
      "Is it sarcastic 0, Confidence: 0.9446668028831482\n",
      "Is it sarcastic 0, Confidence: 0.9918295741081238\n",
      "Is it sarcastic 0, Confidence: 0.9519305229187012\n",
      "Is it sarcastic 1, Confidence: 0.5032348036766052\n",
      "Is it sarcastic 0, Confidence: 0.582430899143219\n",
      "Is it sarcastic 0, Confidence: 0.9851067066192627\n",
      "Is it sarcastic 0, Confidence: 0.6958474516868591\n",
      "Is it sarcastic 0, Confidence: 0.8845799565315247\n"
     ]
    }
   ],
   "source": [
    "# sarcastic_samples = sarcastic_samples_neg + sarcastic_samples_pos\n",
    "\n",
    "# for review in sarcastic_samples:\n",
    "    \n",
    "#     text = review[0]\n",
    "\n",
    "#     tokenized_text = tokenizer([preprocess_data(text)], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "#     output = model(**tokenized_text)\n",
    "\n",
    "#     probs = output.logits.softmax(dim=-1).tolist()[0]\n",
    "#     confidence = max(probs)\n",
    "\n",
    "#     prediction = probs.index(confidence)\n",
    "#     print(f\"Is it sarcastic {prediction}, Confidence: {confidence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
