{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c860b1-0b6d-4477-90f7-fef83b157e14",
   "metadata": {},
   "source": [
    "# Replication of COCKATIEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35250c7d-a438-49d7-990e-329237bc44f2",
   "metadata": {},
   "source": [
    "## Prepare RoBERTa Model\n",
    "\n",
    "RoBERTa model with **a non-negative (ReLU)** last layer to discover concepts using COCKATIEL.\n",
    "We use the `features` method to compute the activations on which to apply the NMF, `end_model` to go\n",
    "from these activations to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ff2934-2ffb-4c38-a31a-4b95fa53c540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss\n",
    "from transformers import RobertaPreTrainedModel, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fd31f-eac7-43a2-aa05-9b4b829f0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom fully-connected classification head for RoBERTa with a non-negative layer on which we can compute the NMF.\n",
    "\n",
    "class CustomRobertaClassificationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x) # Projection layer, yani multi-head attention katmanının yüksek boyutlu output'unu düşürmek için project yapar\n",
    "                             # Ardından final sınıflandırma logits'leri hesaplanır\n",
    "        return x\n",
    "\n",
    "    # out_proj olmadan feature'ları hesaplayan fonk.\n",
    "    def features(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "    # forward()'ın sonunda yapılan işlemin aynısı, ayrı fonk. olarak da tanımlamışlar\n",
    "    def end_model(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ee863-8c8b-42a9-b805-b292ffc72c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom RoBERTa model using a custom fully-connected head with a non-negative layer on which we can compute the NMF.\n",
    "    \n",
    "class CustomRobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    # Model yüklenirken position_ids'i ignore etsin\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Regresyon problemi gibi ele aldıklarından 1 demişler, yoksa sentiment a. için 2 olmalıydı\n",
    "        self.num_labels = 1\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = CustomRobertaClassificationHead(config)\n",
    "        \n",
    "        # Yine regresyon olmasından dolayı\n",
    "        self.mse_loss = MSELoss()\n",
    "\n",
    "        # Modelin başka hiperparametrelerini initialize etmek için ek fonk.\n",
    "        self.post_init()\n",
    "\n",
    "    def features(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "    ):\n",
    "        features = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return self.classifier.features(features[0][:, 0, :])\n",
    "\n",
    "    def end_model(self, activations):\n",
    "        return self.classifier.end_model(activations)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f03bc3-88d3-4cd0-b5e6-9edfcdaac66b",
   "metadata": {},
   "source": [
    "## COCKATIEL: Explainability Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0174a-c28c-4780-a12f-ad30467e4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import NMF\n",
    "from math import ceil\n",
    "\n",
    "from .sampling import ScipySobolSequence, concept_perturbation\n",
    "from .sobol import JansenEstimator\n",
    "from .utils import tokenize\n",
    "\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9e5ad-3163-4a25-beba-171dd732847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCKATIEL:\n",
    "    \"\"\"\n",
    "    model\n",
    "        The Torch hugging-face model that we wish to explain. It MUST have a non-negative layer on\n",
    "        which to extract the concepts.\n",
    "    tokenizer\n",
    "        A callable object to transform strings into inputs for the model.\n",
    "    components\n",
    "        An integer for the amount of concepts we wish to discover in the activation space.\n",
    "    batch_size\n",
    "        The batch size for all the operations that use the model\n",
    "    device\n",
    "        The type of device on which to place the torch tensors\n",
    "    \"\"\"\n",
    "    sobol_nb_design = 32\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            tokenizer: Callable,\n",
    "            components: int = 25,\n",
    "            batch_size: int = 256,\n",
    "            device: str = 'cuda',\n",
    "            nmf_max_iter: int = 1000\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.components = components\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.max_iter = nmf_max_iter\n",
    "\n",
    "    def extract_concepts(\n",
    "            self,\n",
    "            cropped_dataset: List[str],\n",
    "            dataset: List[str],\n",
    "            class_id: int,\n",
    "            limit_sobol: int = 1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extracts the concepts following the object's parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cropped_dataset\n",
    "            The dataset containing the excerpts used to discover the concepts.\n",
    "        dataset\n",
    "            A sample of the dataset (with whole inputs) on which to compute the Sobol importance.\n",
    "        class_id\n",
    "            An integer for the class we wish to explain.\n",
    "        limit_sobol\n",
    "            The maximum amount of masks to use for estimating Sobol indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        excerpts\n",
    "            The excerpts used to learn the concepts.\n",
    "        u_excerpts\n",
    "            The coefficients in the learned concept base for the excerpts.\n",
    "        factorization\n",
    "            The object to transform activations using the concept base.\n",
    "        global_importance\n",
    "            An array with the global importance of each concept (Sobol indices).\n",
    "        \"\"\"\n",
    "        excerpts = []\n",
    "        excerpts_activations = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_id in range(ceil(len(cropped_dataset) / self.batch_size)):\n",
    "                batch_start = batch_id * self.batch_size\n",
    "                batch_end = batch_start + self.batch_size\n",
    "\n",
    "                batch_sentences = cropped_dataset[batch_start:batch_end]\n",
    "                batch_tokenized = tokenize(batch_sentences, self.tokenizer, self.device)\n",
    "\n",
    "                batch_activations = self.model.features(**batch_tokenized)\n",
    "\n",
    "                excerpts_activations = batch_activations if excerpts_activations is None \\\n",
    "                    else torch.cat([excerpts_activations, batch_activations], 0)\n",
    "                excerpts = batch_sentences if excerpts is None \\\n",
    "                    else excerpts + batch_sentences\n",
    "\n",
    "        points_activations = None\n",
    "        with torch.no_grad():\n",
    "            for batch_id in range(ceil(len(dataset) / self.batch_size)):\n",
    "                batch_start = batch_id * self.batch_size\n",
    "                batch_end = batch_start + self.batch_size\n",
    "                tokenized_batch = tokenize(dataset[batch_start:batch_end], self.tokenizer, self.device)\n",
    "                activations = self.model.features(**tokenized_batch)\n",
    "                points_activations = activations if points_activations is None else torch.cat(\n",
    "                    [points_activations, activations])\n",
    "\n",
    "        # applying GAP(.) on the activation and ensure positivity if needed\n",
    "        excerpts_activations = self._preprocess(excerpts_activations)\n",
    "        points_activations = self._preprocess(points_activations)\n",
    "\n",
    "        # using the activations, we will now use the matrix factorization to\n",
    "        # find the concept bank (W) and the concept representation (U) of the\n",
    "        # segments and the points\n",
    "        factorization = NMF(n_components=self.components, max_iter=self.max_iter)\n",
    "\n",
    "        u_excerpts = factorization.fit_transform(excerpts_activations)\n",
    "        W = torch.Tensor(factorization.components_).float().to(self.device)\n",
    "\n",
    "        # we don't need segments activations anymore, the concept bank is trained\n",
    "        del excerpts_activations\n",
    "\n",
    "        # using the concept bank and the points, we will now evaluate the importance of\n",
    "        # each concept for each points to get a global importance score for each\n",
    "        # concept in the concept bank\n",
    "        global_importance = self._sobol_importance(cropped_dataset, points_activations[:limit_sobol], class_id, W)\n",
    "\n",
    "        return excerpts, u_excerpts, factorization, global_importance\n",
    "\n",
    "    def _sobol_importance(self, cropped_dataset, activations: torch.Tensor, class_id: int, W: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Computes the Sobol indices using the dataset containing the excerpts and the activations from the\n",
    "        dataset (whole inputs) for the target class, and for a fixed (already learned) concept base W.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cropped_dataset\n",
    "            The activations of the dataset containing the excerpts used to discover the concepts.\n",
    "        activations\n",
    "            The activations for inputs from the original dataset.\n",
    "        class_id\n",
    "            An integer for the class we wish to explain.\n",
    "        W\n",
    "            The (already learned) concept base.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        global_importance\n",
    "            An array with the Sobol indices\n",
    "        \"\"\"\n",
    "        masks = ScipySobolSequence()(self.components, nb_design=self.sobol_nb_design)\n",
    "        estimator = JansenEstimator()\n",
    "\n",
    "        if not isinstance(W, torch.Tensor):\n",
    "            W = torch.Tensor(W).float().to(self.device)\n",
    "\n",
    "        importances = []\n",
    "        for act in activations:\n",
    "            act = torch.Tensor(act).float().to(self.device)\n",
    "\n",
    "            y_pred = None\n",
    "            for batch_id in range(ceil(len(cropped_dataset) / self.batch_size)):\n",
    "                batch_start = batch_id * self.batch_size\n",
    "                batch_end = batch_start + self.batch_size\n",
    "                batch_masks = torch.Tensor(masks[batch_start:batch_end]).float().to(self.device)\n",
    "\n",
    "                y_batch = concept_perturbation(self.model, act, batch_masks, class_id, W)\n",
    "                y_pred = y_batch if y_pred is None else torch.cat([y_pred, y_batch], 0)\n",
    "\n",
    "            if self.device == 'cuda' or self.device == torch.device('cuda'):\n",
    "                y_pred = y_pred.cpu()\n",
    "            stis = estimator(masks, y_pred.numpy(), self.sobol_nb_design)\n",
    "            importances.append(stis)\n",
    "\n",
    "        global_importance = np.mean(importances, 0)\n",
    "\n",
    "        return global_importance\n",
    "\n",
    "    def _preprocess(self, activations: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Preprocesses the activations to make sure that they're the right shape for being input to the\n",
    "        NMF algorithm later.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations\n",
    "            The (non-negative) activations from the model under study.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        activations\n",
    "            The preprocessed activations, ready for COCKATIEL.\n",
    "        \"\"\"\n",
    "        if len(activations.shape) == 4:\n",
    "            activations = torch.mean(activations, (1, 2))\n",
    "\n",
    "        if torch.min(activations) < 0:\n",
    "            raise ValueError(\"Please choose a layer with positive activations.\")\n",
    "        if self.device == 'cuda' or self.device == torch.device('cuda'):\n",
    "            activations = activations.cpu()\n",
    "\n",
    "        return activations.numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab1eeb-b4f2-42b2-8aff-3e93f541b40a",
   "metadata": {},
   "source": [
    "## Occlusion: Local Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ad03-0a0d-48a7-b3bb-3a011eefc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn.decomposition\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from .utils import extract_clauses, calculate_u_values, calculate_importance\n",
    "\n",
    "from typing import List, Callable, Optional, Union, Tuple\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b37f86-2f3d-46d2-a14f-213bf65766f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements the local part of COCKATIEL: occlusion. This allows us to estimate the presence of\n",
    "concepts in parts of the input text.\n",
    "\"\"\"\n",
    "\n",
    "def occlusion_concepts(\n",
    "        sentence: str,\n",
    "        model,\n",
    "        tokenizer: Callable,\n",
    "        factorization: Union[sklearn.decomposition.NMF, Tuple[sklearn.decomposition.NMF, sklearn.decomposition.NMF]],\n",
    "        l_concept_id: Union[np.ndarray, Tuple[np.ndarray, np.ndarray]],\n",
    "        ignore_words: Optional[List[str]] = None,\n",
    "        two_labels: bool = True,\n",
    "        extract_fct: str = \"clause\",\n",
    "        device='cuda'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates explanations for the input sentence using COCKATIEL.\n",
    "\n",
    "    If two_labels is False, it computes the presence of the concepts of interest (in l_concept_id) using the\n",
    "    NMF object in factorization.\n",
    "    If two_labels is True, it computes the presence of the concepts of interest in the tuple of l_concept_id using\n",
    "    the tuple of NMF objects in factorization (to do so for both classes in imdb-reviews task).\n",
    "\n",
    "    The granularity of the explanations is set with extract_fct.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence\n",
    "        The string (sentence) we wish to explain using COCKATIEL.\n",
    "    model\n",
    "        The model under study.\n",
    "    tokenizer\n",
    "        A Callable that transforms strings into tokens capable of being ingested by the model.\n",
    "    factorization\n",
    "        Either the NMF object to transform activations into the concept base, or a tuple with an object for each of\n",
    "        the two classes (for the imdb-reviews task) if two_labels is True.\n",
    "    l_concept_id\n",
    "        Either a list of concepts of interest (for a given task) or a tuple with two lists, one for each class (if\n",
    "        two_labels is True).\n",
    "    ignore_words\n",
    "        A list of strings to ignore when applying occlusion.\n",
    "    two_labels\n",
    "        A bool indicating whether we wish to explain only one class or both (for imdb-reviews task).\n",
    "    extract_fct\n",
    "        A string indicating whether at which level we wish to explain: \"word\", \"clause\" or \"sentence\".\n",
    "    device\n",
    "        The device on which tensors are stored (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    l_importances\n",
    "        An array with the presence of each concept in the input sentence.\n",
    "    \"\"\"\n",
    "    sentence = str(sentence)\n",
    "\n",
    "    if extract_fct == \"clause\":\n",
    "        words = extract_clauses(sentence, clause_type=None)\n",
    "        separate = \" \"\n",
    "\n",
    "    else:\n",
    "        words = word_tokenize(sentence)\n",
    "        if extract_fct == \"sentence\":\n",
    "            separate = \". \"\n",
    "        elif extract_fct == \"word\":\n",
    "            separate = \" \"\n",
    "        else:\n",
    "            raise ValueError(\"Extraction function can be only 'clause', 'sentence', or 'word\")\n",
    "\n",
    "    if two_labels:\n",
    "        u_values_pos = calculate_u_values(sentence, words,  model, tokenizer, factorization[0], separate, ignore_words, device)\n",
    "        u_values_neg = calculate_u_values(sentence, words,  model, tokenizer, factorization[1], separate, ignore_words, device)\n",
    "        l_importances = []\n",
    "        for concept_id in l_concept_id[0]:\n",
    "            importances = calculate_importance(words, u_values_pos, concept_id, ignore_words)\n",
    "            l_importances.append(np.array(importances))\n",
    "        for concept_id in l_concept_id[1]:\n",
    "            importances = calculate_importance(words, u_values_neg, concept_id, ignore_words)\n",
    "            l_importances.append(np.array(importances))\n",
    "\n",
    "    else:  # look at only one class:\n",
    "        u_values = calculate_u_values(sentence, words,  model, tokenizer, factorization, separate, ignore_words, device)\n",
    "        l_importances = []\n",
    "        for concept_id in l_concept_id:\n",
    "            importances = calculate_importance(words, u_values, concept_id, ignore_words)\n",
    "            l_importances.append(np.array(importances))\n",
    "\n",
    "    return np.array(l_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b56870-d429-4ad9-a884-7938d2a13088",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4a453-78db-4c7a-b560-0ac801dc7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9aaead-69af-4f2e-8472-9db04a11a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sampling methods for replicated designs\n",
    "\"\"\"\n",
    "\n",
    "class Sampler(ABC):\n",
    "    \"\"\"\n",
    "    Base class for replicated design sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build_replicated_design(sampling_a, sampling_b):\n",
    "        \"\"\"\n",
    "        Build the replicated design matrix C using A & B\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sampling_a\n",
    "          The masks values for the sampling matrix A.\n",
    "        sampling_b\n",
    "          The masks values for the sampling matrix B.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        replication_c\n",
    "          The new replicated design matrix C generated from A & B.\n",
    "        \"\"\"\n",
    "        replication_c = np.array([sampling_a.copy() for _ in range(sampling_a.shape[-1])])\n",
    "        for i in range(len(replication_c)):\n",
    "            replication_c[i, :, i] = sampling_b[:, i]\n",
    "\n",
    "        replication_c = replication_c.reshape((-1, sampling_a.shape[-1]))\n",
    "\n",
    "        return replication_c\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, dimension, nb_design):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ScipySampler(Sampler):\n",
    "    \"\"\"\n",
    "    Base class based on Scipy qmc module for replicated design sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.qmc = scipy.stats.qmc\n",
    "        except AttributeError as err:\n",
    "            raise ModuleNotFoundError(\"COCKATIEL need scipy>=1.7 to use this sampling.\") from err\n",
    "\n",
    "\n",
    "class ScipySobolSequence(ScipySampler):\n",
    "    \"\"\"\n",
    "    Scipy Sobol LP tau sequence sampler.\n",
    "\n",
    "    Ref. I. M. Sobol., The distribution of points in a cube and the accurate evaluation of\n",
    "    integrals (1967).\n",
    "    https://www.sciencedirect.com/science/article/abs/pii/0041555367901449\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, dimension, nb_design):\n",
    "        sampler = self.qmc.Sobol(dimension * 2, scramble=False)\n",
    "        sampling_ab = sampler.random(nb_design).astype(np.float32)\n",
    "        sampling_a, sampling_b = sampling_ab[:, :dimension], sampling_ab[:, dimension:]\n",
    "        replicated_c = self.build_replicated_design(sampling_a, sampling_b)\n",
    "\n",
    "        return np.concatenate([sampling_a, sampling_b, replicated_c], 0)\n",
    "\n",
    "\n",
    "def concept_perturbation(model, activation, masks, class_id, W):\n",
    "    \"\"\"\n",
    "    Apply perturbation on the concept before reconstruction and get the perturbated outputs.\n",
    "    For NMF we recall that A = U @ W\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "      Model that map the concept layer to the output (h_l->k in the paper)\n",
    "    activation\n",
    "      Specific activation to apply perturbation on.\n",
    "    masks\n",
    "      Arrays of masks, each of them being a concept perturbation.\n",
    "    class_id\n",
    "      Id the class to test.\n",
    "    W\n",
    "      Concept bank extracted using NMF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y\n",
    "      Outputs of the perturbated points.\n",
    "    \"\"\"\n",
    "    perturbation = masks @ W\n",
    "\n",
    "    if len(activation.shape) == 3:\n",
    "        perturbation = perturbation[:, None, None, :]\n",
    "\n",
    "    activation = activation[None, :]\n",
    "    perturbated_activations = activation + perturbation * activation\n",
    "    y = model.end_model(perturbated_activations)[:, class_id]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226cdd3-9500-48b2-bdec-d0cc2e5e0a3f",
   "metadata": {},
   "source": [
    "## Total Sobol Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c9f93-2002-4c80-9530-47a611f530a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e8ef8-560e-4471-a228-02724b8c3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sobol' total order estimators module\n",
    "\"\"\"\n",
    "\n",
    "class SobolEstimator(ABC):\n",
    "    \"\"\"\n",
    "    Base class for Sobol' total order estimators.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def masks_dim(masks):\n",
    "        \"\"\"\n",
    "        Deduce the number of dimensions using the sampling masks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        masks\n",
    "          Low resolution masks (before upsampling) used, one for each output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nb_dim\n",
    "          The number of dimensions under study according to the masks.\n",
    "        \"\"\"\n",
    "        nb_dim = np.prod(masks.shape[1:])\n",
    "        return nb_dim\n",
    "\n",
    "    @staticmethod\n",
    "    def split_abc(outputs, nb_design, nb_dim):\n",
    "        \"\"\"\n",
    "        Split the outputs values into the 3 sampling matrices A, B and C.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs\n",
    "          Model outputs for each sample point of matrices A, B and C (in order).\n",
    "        nb_design\n",
    "          Number of points for matrices A (the same as B).\n",
    "        nb_dim\n",
    "          Number of dimensions to estimate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a\n",
    "          The results for the sample points in matrix A.\n",
    "        b\n",
    "          The results for the sample points in matrix B.\n",
    "        c\n",
    "          The results for the sample points in matrix C.\n",
    "        \"\"\"\n",
    "        sampling_a = outputs[:nb_design]\n",
    "        sampling_b = outputs[nb_design:nb_design*2]\n",
    "        replication_c = np.array([outputs[nb_design*2 + nb_design*i:nb_design*2 + nb_design*(i+1)]\n",
    "                      for i in range(nb_dim)])\n",
    "        return sampling_a, sampling_b, replication_c\n",
    "\n",
    "    @staticmethod\n",
    "    def post_process(stis, masks):\n",
    "        \"\"\"\n",
    "        Post processing ops on the indices before sending them back. Makes sure the data\n",
    "        format and shape is correct.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stis\n",
    "          Total order Sobol' indices, one for each dimensions.\n",
    "        masks\n",
    "            Low resolution masks (before upsampling) used, one for each output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stis\n",
    "          Total order Sobol' indices after post processing.\n",
    "        \"\"\"\n",
    "        stis = np.array(stis, np.float32)\n",
    "        return stis.reshape(masks.shape[1:])\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, masks, outputs, nb_design):\n",
    "        \"\"\"\n",
    "        Compute the Sobol' total order indices according to the Jansen algorithm.\n",
    "\n",
    "        Ref. Jansen, M., Analysis of variance designs for model output (1999)\n",
    "        https://www.sciencedirect.com/science/article/abs/pii/S0010465598001544\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        masks\n",
    "          Low resolution masks (before upsampling) used, one for each output.\n",
    "        outputs\n",
    "          Model outputs associated to each masks. One for each sample point of\n",
    "          matrices A, B and C (in order).\n",
    "        nb_design\n",
    "          Number of points for matrices A (the same as B).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sti: ndarray\n",
    "          Total order Sobol' indices, one for each dimensions.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class JansenEstimator(SobolEstimator):\n",
    "    \"\"\"\n",
    "    Jansen estimator for total order Sobol' indices.\n",
    "\n",
    "    Ref. Jansen, M., Analysis of variance designs for model output (1999)\n",
    "    https://www.sciencedirect.com/science/article/abs/pii/S0010465598001544\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, masks, outputs, nb_design):\n",
    "        \"\"\"\n",
    "        Compute the Sobol' total order indices according to the Jansen algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        masks\n",
    "          Low resolution masks (before upsampling) used, one for each output.\n",
    "        outputs\n",
    "          Model outputs associated to each masks. One for each sample point of\n",
    "          matrices A, B and C (in order).\n",
    "        nb_design\n",
    "          Number of points for matrices A (the same as B).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sti\n",
    "          Total order Sobol' indices, one for each dimensions.\n",
    "        \"\"\"\n",
    "        nb_dim = self.masks_dim(masks)\n",
    "        sampling_a, _, replication_c = self.split_abc(outputs, nb_design, nb_dim)\n",
    "\n",
    "        mu_a = np.mean(sampling_a)\n",
    "        var = np.sum([(v - mu_a)**2 for v in sampling_a]) / (len(sampling_a) - 1)\n",
    "\n",
    "        stis = [\n",
    "            np.sum((sampling_a - replication_c[i])**2.0) / (2 * nb_design * var)\n",
    "            for i in range(nb_dim)\n",
    "        ]\n",
    "\n",
    "        return self.post_process(stis, masks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333235f-09af-45bb-87f7-03e8e95c58f5",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4de6c-8ef6-48e3-adc6-0e93da225d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "from typing import List, Callable, Union, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b92c52-74f2-4986-b3bf-2f1cff126645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/chunk-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e9c67-f310-4a9d-bea8-528645240bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(elements, batch_size: int):\n",
    "    \"\"\"\n",
    "    An iterable to create batches from a list of elements\n",
    "    \"\"\"\n",
    "    nb_batchs = ceil(len(elements) / batch_size)\n",
    "\n",
    "    for batch_i in range(nb_batchs):\n",
    "        batch_start = batch_i * batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "\n",
    "        batch = elements[batch_start:batch_end]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340df04-e033-4293-b33c-d6892206c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples: List[str], tokenizer: Callable, device='cuda'):\n",
    "    \"\"\"\n",
    "    A function to transform a list of strings into tokens to be consumed by the transformer model.\n",
    "    \"\"\"\n",
    "    x = tokenizer(\n",
    "        [s for s in samples],\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd6b2a-8460-4e08-bfe0-7f17893beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(samples: List[Tuple[str, str]], tokenizer: Callable, device='cuda'):\n",
    "    \"\"\"\n",
    "    A basic pre-processing function to transform the format from the imdb dataset to\n",
    "    something easier to work with.\n",
    "    \"\"\"\n",
    "    x, y = samples[:, 0], samples[:, 1]\n",
    "    x = tokenize(x, tokenizer, device)\n",
    "    y = torch.Tensor(y == 'positive').int().to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607d5d5-c494-4d7d-8af2-afc361b8793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(model, tokenizer: Callable, inputs: List[Tuple[str, str]], batch_size: int = 64, device='cuda'):\n",
    "    \"\"\"\n",
    "    A function to pre-process and predict using the transformer model in batches.\n",
    "    \"\"\"\n",
    "    predictions = None\n",
    "    labels = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_input in batcher(inputs, batch_size):\n",
    "            xp, yp = preprocess(batch_input, tokenizer, device)\n",
    "            out_batch = model(**xp)\n",
    "            predictions = out_batch if predictions is None else torch.cat([predictions, out_batch])\n",
    "            labels = yp if labels is None else torch.cat([labels, yp])\n",
    "\n",
    "        return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65100287-5bce-4026-a92a-db44aac46856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clauses(ds_entry: Union[List[str], str], clause_type=['NP', 'ADJP']) -> List[str]:\n",
    "    \"\"\"\n",
    "    Separates the input texts into clauses, and only keeps the ones belonging to the specified types.\n",
    "    If clause_type is None, the texts are split but all the clauses are kept.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_entry\n",
    "        A list of strings that we wish to separate into clauses.\n",
    "    clause_type\n",
    "        A list with the types of clauses to keep. If None, all clauses are kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clause_list\n",
    "        A list with input texts split into clauses.\n",
    "    \"\"\"\n",
    "    s = Sentence(ds_entry)\n",
    "    tagger.predict(s)\n",
    "    clause_list = []\n",
    "    for segment in s.get_labels():\n",
    "        if clause_type is None:\n",
    "            clause_list.append(segment.data_point.text)\n",
    "        elif segment.value in clause_type:\n",
    "            clause_list.append(segment.data_point.text)\n",
    "\n",
    "    return clause_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e68775-9be7-41f1-bc07-9dd3e2816eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_activations_fct(model, inputs: List[str], batch_size=64) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A function to extract the activations of input texts in batches.\n",
    "    \"\"\"\n",
    "    activations = None\n",
    "    with torch.no_grad():\n",
    "        for batch_input in batcher(inputs, batch_size):\n",
    "            out_batch = model.features(**batch_input)\n",
    "            activations = out_batch if activations is None else torch.cat([activations, out_batch])\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593c865-ac3d-44fb-b598-14d0ec4c4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acti_preprocess(activations: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A function to preprocess the activations to work with COCKATIEL\n",
    "    \"\"\"\n",
    "    if len(activations.shape) == 4:\n",
    "        activations = torch.mean(activations, (1, 2))\n",
    "\n",
    "    if isinstance(activations, np.ndarray):\n",
    "        activations = torch.Tensor(activations)\n",
    "    if torch.min(activations) < 0:\n",
    "        raise ValueError(\"Please choose a layer with positive activations.\")\n",
    "\n",
    "    return activations.cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e55b14-10bd-4561-aa92-1e71de69dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_u_values(sentence, cropped_sentences, model, tokenizer, factorization,\n",
    "                       separate, ignore_words: Optional[List[str]] = None, device='cuda') -> np.ndarray:\n",
    "    if ignore_words is None:\n",
    "        ignore_words = []\n",
    "    with torch.no_grad():\n",
    "        activations = None\n",
    "        for crop_id in range(-1, len(cropped_sentences)):\n",
    "            if crop_id == -1:\n",
    "                perturbated_review = sentence\n",
    "            elif cropped_sentences[crop_id] not in ignore_words:\n",
    "                perturbated_review = separate.join(np.delete(cropped_sentences, crop_id))\n",
    "            else:\n",
    "                continue\n",
    "            tokenized_perturbated_review = tokenizer(perturbated_review, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "            activation = model.features(**tokenized_perturbated_review)\n",
    "            activations = activation if activations is None else torch.cat([activations, activation])\n",
    "\n",
    "        activations = acti_preprocess(activations)\n",
    "        u_values = factorization.transform(activations)\n",
    "        \n",
    "        return u_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3f8ea-1b09-4c7b-a290-bcc60270f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance(\n",
    "        words: List[str], u_values: np.ndarray, concept_id: int, ignore_words: List[str]\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculates the presence of concepts in the input list of words.\n",
    "    \"\"\"\n",
    "    u_delta = u_values[0, concept_id] - u_values[1:, concept_id]\n",
    "    importances = []\n",
    "    delta_id = 0  # pointer to get current id in importance (as we skip unused word)\n",
    "\n",
    "    for word_id in range(len(words)):\n",
    "        if words[word_id] not in ignore_words:\n",
    "            importances.append(u_delta[delta_id])\n",
    "            delta_id += 1\n",
    "        else:\n",
    "            importances.append(0.0)\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4707b0-4508-4df0-9351-a66081fa5957",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a4a71-8538-4f76-a444-d0a2f4b7eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from .utils import extract_clauses\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "nltk.download('punkt')\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa714f3-ee46-422d-b6a3-44b537f8b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module for generating plots for the explanations on notebooks.\n",
    "\"\"\"\n",
    "\n",
    "def print_legend(colors, label_to_criterion):\n",
    "    \"\"\"\n",
    "    Prints the legend for the plot in different colors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    colors\n",
    "        A dictionary with the colors for each label.\n",
    "    label_to_criterion\n",
    "        A dictionary with the text to put on each label.\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for label_id in label_to_criterion.keys():\n",
    "        html.append(f'<span style=\"background-color: {colors[label_id]} 0.5); padding: 1px 5px; border: solid 3px ; border-color: {colors[label_id]} 1); #EFEFEF\">{label_to_criterion[label_id]} </span>')\n",
    "    display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(html) + \" </div>\" ))\n",
    "    display(HTML('<br><br>'))\n",
    "\n",
    "\n",
    "def viz_concepts(\n",
    "        text,\n",
    "        explanation,\n",
    "        colors,\n",
    "        ignore_words: Optional[List[str]] = None,\n",
    "        extract_fct: str = \"clause\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the visualization for COCKATIEL's explanations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text\n",
    "        A string with the text we wish to explain.\n",
    "    explanation\n",
    "        An array that corresponds to the output of the occlusion function.\n",
    "    ignore_words\n",
    "        A list of strings to ignore when applying occlusion.\n",
    "    extract_fct\n",
    "        A string indicating whether at which level we wish to explain: \"word\", \"clause\" or \"sentence\".\n",
    "    colors\n",
    "        A dictionary with the colors for each label\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.decode('utf-8')\n",
    "    except:\n",
    "        text = str(text)\n",
    "\n",
    "    if extract_fct == \"clause\":\n",
    "        words = extract_clauses(text, clause_type=None)\n",
    "    else:\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "    l_phi = np.array(explanation)\n",
    "\n",
    "    phi_html = []\n",
    "\n",
    "    p = 0  # pointer to get current color for the words (it does not color words that have no phi)\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in ignore_words:\n",
    "            k = 0\n",
    "            for j in range(len(l_phi)):\n",
    "                if l_phi[k][p] < l_phi[j][p]:\n",
    "                    k = j\n",
    "\n",
    "            if l_phi[k][p] > 0.2:\n",
    "                phi_html.append(f'<span style=\"background-color: {colors[k]} {l_phi[k][p]}); padding: 1px 5px; border: solid 3px ; border-color: {colors[k]} 1); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "            else:\n",
    "                phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "        else:\n",
    "            phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "    display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(phi_html) + \" </div>\" ))\n",
    "    display(HTML('<br><br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46954b6c-555f-4bfd-b2bb-38ce49e8d47e",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf45076-056d-4f89-b13e-a39360f499b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563d795-88f5-44dd-8440-c86ff316d6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afee32c-b00a-444d-8ee2-87c01e89a223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304339c-1fa6-4358-a01b-e4fee8b1870d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cockatiel",
   "language": "python",
   "name": "cockatiel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
