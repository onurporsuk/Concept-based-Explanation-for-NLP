{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c860b1-0b6d-4477-90f7-fef83b157e14",
   "metadata": {},
   "source": [
    "# Replication of COCKATIEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35250c7d-a438-49d7-990e-329237bc44f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare RoBERTa Model\n",
    "\n",
    "RoBERTa model with **a non-negative (ReLU)** last layer to discover concepts using COCKATIEL.\n",
    "We use the `features` method to compute the activations on which to apply the NMF, `end_model` to go\n",
    "from these activations to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fd31f-eac7-43a2-aa05-9b4b829f0224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss\n",
    "from transformers import RobertaPreTrainedModel, RobertaModel\n",
    "\n",
    "\n",
    "\n",
    "# A custom fully-connected classification head for RoBERTa with a non-negative layer on which we can compute the NMF.\n",
    "\n",
    "class CustomRobertaClassificationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x) # Projection layer, yani multi-head attention katmanının yüksek boyutlu output'unu düşürmek için project yapar\n",
    "                             # Ardından final sınıflandırma logits'leri hesaplanır\n",
    "        return x\n",
    "\n",
    "    # out_proj olmadan feature'ları hesaplayan fonk.\n",
    "    def features(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "    # forward()'ın sonunda yapılan işlemin aynısı, ayrı fonk. olarak da tanımlamışlar\n",
    "    def end_model(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# A custom RoBERTa model using a custom fully-connected head with a non-negative layer on which we can compute the NMF.\n",
    "    \n",
    "class CustomRobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "\n",
    "    # Model yüklenirken position_ids'i ignore etsin\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Regresyon problemi gibi ele aldıklarından 1 demişler, yoksa sentiment a. için 2 olmalıydı\n",
    "        self.num_labels = 1\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = CustomRobertaClassificationHead(config)\n",
    "        \n",
    "        # Yine regresyon olmasından dolayı\n",
    "        self.mse_loss = MSELoss()\n",
    "\n",
    "        # Modelin başka hiperparametrelerini initialize etmek için ek fonk.\n",
    "        self.post_init()\n",
    "\n",
    "    def features(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "    ):\n",
    "        features = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return self.classifier.features(features[0][:, 0, :])\n",
    "\n",
    "    def end_model(self, activations):\n",
    "        return self.classifier.end_model(activations)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f03bc3-88d3-4cd0-b5e6-9edfcdaac66b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COCKATIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9e5ad-3163-4a25-beba-171dd732847f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import NMF\n",
    "from math import ceil\n",
    "from typing import Callable, List\n",
    "\n",
    "\n",
    "class COCKATIEL:\n",
    "    \"\"\"\n",
    "    model\n",
    "        The Torch hugging-face model that we wish to explain. It MUST have a non-negative layer on\n",
    "        which to extract the concepts.\n",
    "    tokenizer\n",
    "        A callable object to transform strings into inputs for the model.\n",
    "    components\n",
    "        An integer for the amount of concepts we wish to discover in the activation space.\n",
    "    batch_size\n",
    "        The batch size for all the operations that use the model\n",
    "    device\n",
    "        The type of device on which to place the torch tensors\n",
    "    \"\"\"\n",
    "    sobol_nb_design = 32\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            tokenizer: Callable,\n",
    "            components: int = 25,\n",
    "            batch_size: int = 256,\n",
    "            device: str = 'cuda',\n",
    "            nmf_max_iter: int = 1000\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.components = components\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.max_iter = nmf_max_iter\n",
    "\n",
    "    def extract_concepts(\n",
    "            self,\n",
    "            cropped_dataset: List[str],\n",
    "            dataset: List[str],\n",
    "            class_id: int,\n",
    "            limit_sobol: int = 1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extracts the concepts following the object's parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cropped_dataset\n",
    "            The dataset containing the excerpts used to discover the concepts.\n",
    "        dataset\n",
    "            A sample of the dataset (with whole inputs) on which to compute the Sobol importance.\n",
    "        class_id\n",
    "            An integer for the class we wish to explain.\n",
    "        limit_sobol\n",
    "            The maximum amount of masks to use for estimating Sobol indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        excerpts\n",
    "            The excerpts used to learn the concepts.\n",
    "        u_excerpts\n",
    "            The coefficients in the learned concept base for the excerpts.\n",
    "        factorization\n",
    "            The object to transform activations using the concept base.\n",
    "        global_importance\n",
    "            An array with the global importance of each concept (Sobol indices).\n",
    "        \"\"\"\n",
    "        excerpts = []\n",
    "        excerpts_activations = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_id in range(ceil(len(cropped_dataset) / self.batch_size)):\n",
    "                \n",
    "                batch_start = batch_id * self.batch_size\n",
    "                batch_end = batch_start + self.batch_size\n",
    "\n",
    "                batch_sentences = cropped_dataset[batch_start:batch_end]\n",
    "                batch_tokenized = tokenize(batch_sentences, self.tokenizer, self.device)\n",
    "\n",
    "                batch_activations = self.model.features(**batch_tokenized)\n",
    "\n",
    "                excerpts_activations = batch_activations if excerpts_activations is None \\\n",
    "                    else torch.cat([excerpts_activations, batch_activations], 0)\n",
    "                excerpts = batch_sentences if excerpts is None \\\n",
    "                    else excerpts + batch_sentences\n",
    "\n",
    "        points_activations = None\n",
    "        with torch.no_grad():\n",
    "            for batch_id in range(ceil(len(dataset) / self.batch_size)):\n",
    "                \n",
    "                batch_start = batch_id * self.batch_size\n",
    "                batch_end = batch_start + self.batch_size\n",
    "                \n",
    "                tokenized_batch = tokenize(dataset[batch_start:batch_end], self.tokenizer, self.device)\n",
    "                activations = self.model.features(**tokenized_batch)\n",
    "                points_activations = activations if points_activations is None else torch.cat(\n",
    "                    [points_activations, activations])\n",
    "\n",
    "        # applying GAP(.) on the activation and ensure positivity if needed\n",
    "        excerpts_activations = self._preprocess(excerpts_activations)\n",
    "        points_activations = self._preprocess(points_activations)\n",
    "\n",
    "        # using the activations, we will now use the matrix factorization to\n",
    "        # find the concept bank (W) and the concept representation (U) of the\n",
    "        # segments and the points\n",
    "        factorization = NMF(n_components=self.components, max_iter=self.max_iter)\n",
    "\n",
    "        u_excerpts = factorization.fit_transform(excerpts_activations)\n",
    "        W = torch.Tensor(factorization.components_).float().to(self.device)\n",
    "\n",
    "        # we don't need segments activations anymore, the concept bank is trained\n",
    "        del excerpts_activations\n",
    "\n",
    "        # using the concept bank and the points, we will now evaluate the importance of\n",
    "        # each concept for each points to get a global importance score for each\n",
    "        # concept in the concept bank\n",
    "        global_importance = self._sobol_importance(cropped_dataset, points_activations[:limit_sobol], class_id, W)\n",
    "\n",
    "        return excerpts, u_excerpts, factorization, global_importance\n",
    "\n",
    "    def _sobol_importance(self, cropped_dataset, activations: torch.Tensor, class_id: int, W: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Computes the Sobol indices using the dataset containing the excerpts and the activations from the\n",
    "        dataset (whole inputs) for the target class, and for a fixed (already learned) concept base W.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cropped_dataset\n",
    "            The activations of the dataset containing the excerpts used to discover the concepts.\n",
    "        activations\n",
    "            The activations for inputs from the original dataset.\n",
    "        class_id\n",
    "            An integer for the class we wish to explain.\n",
    "        W\n",
    "            The (already learned) concept base.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        global_importance\n",
    "            An array with the Sobol indices\n",
    "        \"\"\"\n",
    "        masks = ScipySobolSequence()(self.components, nb_design=self.sobol_nb_design)\n",
    "        estimator = JansenEstimator()\n",
    "\n",
    "        if not isinstance(W, torch.Tensor):\n",
    "            W = torch.Tensor(W).float().to(self.device)\n",
    "\n",
    "        importances = []\n",
    "        for act in activations:\n",
    "            act = torch.Tensor(act).float().to(self.device)\n",
    "\n",
    "            y_pred = None\n",
    "            for batch_id in range(ceil(len(cropped_dataset) / self.batch_size)):\n",
    "                batch_start = batch_id * self.batch_size\n",
    "                batch_end = batch_start + self.batch_size\n",
    "                batch_masks = torch.Tensor(masks[batch_start:batch_end]).float().to(self.device)\n",
    "\n",
    "                y_batch = concept_perturbation(self.model, act, batch_masks, class_id, W)\n",
    "                y_pred = y_batch if y_pred is None else torch.cat([y_pred, y_batch], 0)\n",
    "\n",
    "            if self.device == 'cuda' or self.device == torch.device('cuda'):\n",
    "                y_pred = y_pred.cpu()\n",
    "            stis = estimator(masks, y_pred.numpy(), self.sobol_nb_design)\n",
    "            importances.append(stis)\n",
    "\n",
    "        global_importance = np.mean(importances, 0)\n",
    "\n",
    "        return global_importance\n",
    "\n",
    "    def _preprocess(self, activations: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Preprocesses the activations to make sure that they're the right shape for being input to the\n",
    "        NMF algorithm later.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations\n",
    "            The (non-negative) activations from the model under study.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        activations\n",
    "            The preprocessed activations, ready for COCKATIEL.\n",
    "        \"\"\"\n",
    "        if len(activations.shape) == 4:\n",
    "            activations = torch.mean(activations, (1, 2))\n",
    "\n",
    "        if torch.min(activations) < 0:\n",
    "            raise ValueError(\"Please choose a layer with positive activations.\")\n",
    "        if self.device == 'cuda' or self.device == torch.device('cuda'):\n",
    "            activations = activations.cpu()\n",
    "\n",
    "        return activations.numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab1eeb-b4f2-42b2-8aff-3e93f541b40a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b37f86-2f3d-46d2-a14f-213bf65766f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "from typing import List, Callable, Optional, Union, Tuple\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This module implements the local part of COCKATIEL: occlusion. This allows us to estimate the presence of\n",
    "concepts in parts of the input text.\n",
    "\"\"\"\n",
    "\n",
    "def occlusion_concepts(\n",
    "        sentence: str,\n",
    "        model,\n",
    "        tokenizer: Callable,\n",
    "        factorization: Union[sklearn.decomposition.NMF, Tuple[sklearn.decomposition.NMF, sklearn.decomposition.NMF]],\n",
    "        l_concept_id: Union[np.ndarray, Tuple[np.ndarray, np.ndarray]],\n",
    "        ignore_words: Optional[List[str]] = None,\n",
    "        two_labels: bool = True,\n",
    "        extract_fct: str = \"clause\",\n",
    "        device='cuda'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates explanations for the input sentence using COCKATIEL.\n",
    "\n",
    "    If two_labels is False, it computes the presence of the concepts of interest (in l_concept_id) using the\n",
    "    NMF object in factorization.\n",
    "    If two_labels is True, it computes the presence of the concepts of interest in the tuple of l_concept_id using\n",
    "    the tuple of NMF objects in factorization (to do so for both classes in imdb-reviews task).\n",
    "\n",
    "    The granularity of the explanations is set with extract_fct.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence\n",
    "        The string (sentence) we wish to explain using COCKATIEL.\n",
    "    model\n",
    "        The model under study.\n",
    "    tokenizer\n",
    "        A Callable that transforms strings into tokens capable of being ingested by the model.\n",
    "    factorization\n",
    "        Either the NMF object to transform activations into the concept base, or a tuple with an object for each of\n",
    "        the two classes (for the imdb-reviews task) if two_labels is True.\n",
    "    l_concept_id\n",
    "        Either a list of concepts of interest (for a given task) or a tuple with two lists, one for each class (if\n",
    "        two_labels is True).\n",
    "    ignore_words\n",
    "        A list of strings to ignore when applying occlusion.\n",
    "    two_labels\n",
    "        A bool indicating whether we wish to explain only one class or both (for imdb-reviews task).\n",
    "    extract_fct\n",
    "        A string indicating whether at which level we wish to explain: \"word\", \"clause\" or \"sentence\".\n",
    "    device\n",
    "        The device on which tensors are stored (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    l_importances\n",
    "        An array with the presence of each concept in the input sentence.\n",
    "    \"\"\"\n",
    "    sentence = str(sentence)\n",
    "\n",
    "    if extract_fct == \"clause\":\n",
    "        words = extract_clauses(sentence, clause_type=None)\n",
    "        separate = \" \"\n",
    "\n",
    "    else:\n",
    "        words = word_tokenize(sentence)\n",
    "        if extract_fct == \"sentence\":\n",
    "            separate = \". \"\n",
    "        elif extract_fct == \"word\":\n",
    "            separate = \" \"\n",
    "        else:\n",
    "            raise ValueError(\"Extraction function can be only 'clause', 'sentence', or 'word\")\n",
    "\n",
    "    if two_labels:\n",
    "        u_values_pos = calculate_u_values(sentence, words,  model, tokenizer, factorization[0], separate, ignore_words, device)\n",
    "        u_values_neg = calculate_u_values(sentence, words,  model, tokenizer, factorization[1], separate, ignore_words, device)\n",
    "        l_importances = []\n",
    "        for concept_id in l_concept_id[0]:\n",
    "            importances = calculate_importance(words, u_values_pos, concept_id, ignore_words)\n",
    "            l_importances.append(np.array(importances))\n",
    "        for concept_id in l_concept_id[1]:\n",
    "            importances = calculate_importance(words, u_values_neg, concept_id, ignore_words)\n",
    "            l_importances.append(np.array(importances))\n",
    "\n",
    "    else:  # look at only one class:\n",
    "        u_values = calculate_u_values(sentence, words,  model, tokenizer, factorization, separate, ignore_words, device)\n",
    "        l_importances = []\n",
    "        for concept_id in l_concept_id:\n",
    "            importances = calculate_importance(words, u_values, concept_id, ignore_words)\n",
    "            l_importances.append(np.array(importances))\n",
    "\n",
    "    return np.array(l_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b56870-d429-4ad9-a884-7938d2a13088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9aaead-69af-4f2e-8472-9db04a11a3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sampling methods for replicated designs\n",
    "\"\"\"\n",
    "\n",
    "class Sampler(ABC):\n",
    "    \"\"\"\n",
    "    Base class for replicated design sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build_replicated_design(sampling_a, sampling_b):\n",
    "        \"\"\"\n",
    "        Build the replicated design matrix C using A & B\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sampling_a\n",
    "          The masks values for the sampling matrix A.\n",
    "        sampling_b\n",
    "          The masks values for the sampling matrix B.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        replication_c\n",
    "          The new replicated design matrix C generated from A & B.\n",
    "        \"\"\"\n",
    "        replication_c = np.array([sampling_a.copy() for _ in range(sampling_a.shape[-1])])\n",
    "        for i in range(len(replication_c)):\n",
    "            replication_c[i, :, i] = sampling_b[:, i]\n",
    "\n",
    "        replication_c = replication_c.reshape((-1, sampling_a.shape[-1]))\n",
    "\n",
    "        return replication_c\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, dimension, nb_design):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ScipySampler(Sampler):\n",
    "    \"\"\"\n",
    "    Base class based on Scipy qmc module for replicated design sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.qmc = scipy.stats.qmc\n",
    "        except AttributeError as err:\n",
    "            raise ModuleNotFoundError(\"COCKATIEL need scipy>=1.7 to use this sampling.\") from err\n",
    "\n",
    "\n",
    "class ScipySobolSequence(ScipySampler):\n",
    "    \"\"\"\n",
    "    Scipy Sobol LP tau sequence sampler.\n",
    "\n",
    "    Ref. I. M. Sobol., The distribution of points in a cube and the accurate evaluation of\n",
    "    integrals (1967).\n",
    "    https://www.sciencedirect.com/science/article/abs/pii/0041555367901449\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, dimension, nb_design):\n",
    "        sampler = self.qmc.Sobol(dimension * 2, scramble=False)\n",
    "        sampling_ab = sampler.random(nb_design).astype(np.float32)\n",
    "        sampling_a, sampling_b = sampling_ab[:, :dimension], sampling_ab[:, dimension:]\n",
    "        replicated_c = self.build_replicated_design(sampling_a, sampling_b)\n",
    "\n",
    "        return np.concatenate([sampling_a, sampling_b, replicated_c], 0)\n",
    "\n",
    "\n",
    "def concept_perturbation(model, activation, masks, class_id, W):\n",
    "    \"\"\"\n",
    "    Apply perturbation on the concept before reconstruction and get the perturbated outputs.\n",
    "    For NMF we recall that A = U @ W\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "      Model that map the concept layer to the output (h_l->k in the paper)\n",
    "    activation\n",
    "      Specific activation to apply perturbation on.\n",
    "    masks\n",
    "      Arrays of masks, each of them being a concept perturbation.\n",
    "    class_id\n",
    "      Id the class to test.\n",
    "    W\n",
    "      Concept bank extracted using NMF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y\n",
    "      Outputs of the perturbated points.\n",
    "    \"\"\"\n",
    "    perturbation = masks @ W\n",
    "\n",
    "    if len(activation.shape) == 3:\n",
    "        perturbation = perturbation[:, None, None, :]\n",
    "\n",
    "    activation = activation[None, :]\n",
    "    perturbated_activations = activation + perturbation * activation\n",
    "    y = model.end_model(perturbated_activations)[:, class_id]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226cdd3-9500-48b2-bdec-d0cc2e5e0a3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Total Sobol Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e8ef8-560e-4471-a228-02724b8c3753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sobol' total order estimators module\n",
    "\"\"\"\n",
    "\n",
    "class SobolEstimator(ABC):\n",
    "    \"\"\"\n",
    "    Base class for Sobol' total order estimators.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def masks_dim(masks):\n",
    "        \"\"\"\n",
    "        Deduce the number of dimensions using the sampling masks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        masks\n",
    "          Low resolution masks (before upsampling) used, one for each output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nb_dim\n",
    "          The number of dimensions under study according to the masks.\n",
    "        \"\"\"\n",
    "        nb_dim = np.prod(masks.shape[1:])\n",
    "        return nb_dim\n",
    "\n",
    "    @staticmethod\n",
    "    def split_abc(outputs, nb_design, nb_dim):\n",
    "        \"\"\"\n",
    "        Split the outputs values into the 3 sampling matrices A, B and C.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs\n",
    "          Model outputs for each sample point of matrices A, B and C (in order).\n",
    "        nb_design\n",
    "          Number of points for matrices A (the same as B).\n",
    "        nb_dim\n",
    "          Number of dimensions to estimate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a\n",
    "          The results for the sample points in matrix A.\n",
    "        b\n",
    "          The results for the sample points in matrix B.\n",
    "        c\n",
    "          The results for the sample points in matrix C.\n",
    "        \"\"\"\n",
    "        sampling_a = outputs[:nb_design]\n",
    "        sampling_b = outputs[nb_design:nb_design*2]\n",
    "        replication_c = np.array([outputs[nb_design*2 + nb_design*i:nb_design*2 + nb_design*(i+1)]\n",
    "                      for i in range(nb_dim)])\n",
    "        return sampling_a, sampling_b, replication_c\n",
    "\n",
    "    @staticmethod\n",
    "    def post_process(stis, masks):\n",
    "        \"\"\"\n",
    "        Post processing ops on the indices before sending them back. Makes sure the data\n",
    "        format and shape is correct.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stis\n",
    "          Total order Sobol' indices, one for each dimensions.\n",
    "        masks\n",
    "            Low resolution masks (before upsampling) used, one for each output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stis\n",
    "          Total order Sobol' indices after post processing.\n",
    "        \"\"\"\n",
    "        stis = np.array(stis, np.float32)\n",
    "        return stis.reshape(masks.shape[1:])\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, masks, outputs, nb_design):\n",
    "        \"\"\"\n",
    "        Compute the Sobol' total order indices according to the Jansen algorithm.\n",
    "\n",
    "        Ref. Jansen, M., Analysis of variance designs for model output (1999)\n",
    "        https://www.sciencedirect.com/science/article/abs/pii/S0010465598001544\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        masks\n",
    "          Low resolution masks (before upsampling) used, one for each output.\n",
    "        outputs\n",
    "          Model outputs associated to each masks. One for each sample point of\n",
    "          matrices A, B and C (in order).\n",
    "        nb_design\n",
    "          Number of points for matrices A (the same as B).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sti: ndarray\n",
    "          Total order Sobol' indices, one for each dimensions.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class JansenEstimator(SobolEstimator):\n",
    "    \"\"\"\n",
    "    Jansen estimator for total order Sobol' indices.\n",
    "\n",
    "    Ref. Jansen, M., Analysis of variance designs for model output (1999)\n",
    "    https://www.sciencedirect.com/science/article/abs/pii/S0010465598001544\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, masks, outputs, nb_design):\n",
    "        \"\"\"\n",
    "        Compute the Sobol' total order indices according to the Jansen algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        masks\n",
    "          Low resolution masks (before upsampling) used, one for each output.\n",
    "        outputs\n",
    "          Model outputs associated to each masks. One for each sample point of\n",
    "          matrices A, B and C (in order).\n",
    "        nb_design\n",
    "          Number of points for matrices A (the same as B).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sti\n",
    "          Total order Sobol' indices, one for each dimensions.\n",
    "        \"\"\"\n",
    "        nb_dim = self.masks_dim(masks)\n",
    "        sampling_a, _, replication_c = self.split_abc(outputs, nb_design, nb_dim)\n",
    "\n",
    "        mu_a = np.mean(sampling_a)\n",
    "        var = np.sum([(v - mu_a)**2 for v in sampling_a]) / (len(sampling_a) - 1)\n",
    "\n",
    "        stis = [\n",
    "            np.sum((sampling_a - replication_c[i])**2.0) / (2 * nb_design * var)\n",
    "            for i in range(nb_dim)\n",
    "        ]\n",
    "\n",
    "        return self.post_process(stis, masks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333235f-09af-45bb-87f7-03e8e95c58f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e9c67-f310-4a9d-bea8-528645240bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from typing import List, Callable, Union, Optional, Tuple\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/chunk-english\")\n",
    "\n",
    "\n",
    "\n",
    "def batcher(elements, batch_size: int):\n",
    "    \"\"\"\n",
    "    An iterable to create batches from a list of elements\n",
    "    \"\"\"\n",
    "    nb_batchs = ceil(len(elements) / batch_size)\n",
    "\n",
    "    for batch_i in range(nb_batchs):\n",
    "        batch_start = batch_i * batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "\n",
    "        batch = elements[batch_start:batch_end]\n",
    "        yield batch\n",
    "        \n",
    "        \n",
    "def tokenize(samples: List[str], tokenizer: Callable, device='cuda'):\n",
    "    \"\"\"\n",
    "    A function to transform a list of strings into tokens to be consumed by the transformer model.\n",
    "    \"\"\"\n",
    "    x = tokenizer(\n",
    "        [s for s in samples],\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess(samples: List[Tuple[str, str]], tokenizer: Callable, device='cuda'):\n",
    "    \"\"\"\n",
    "    A basic pre-processing function to transform the format from the imdb dataset to\n",
    "    something easier to work with.\n",
    "    \"\"\"\n",
    "    x, y = samples[:, 0], samples[:, 1]\n",
    "    x = tokenize(x, tokenizer, device)\n",
    "    y = torch.Tensor(y == 'positive').int().to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def batch_predict(model, tokenizer: Callable, inputs: List[Tuple[str, str]], batch_size: int = 64, device='cuda'):\n",
    "    \"\"\"\n",
    "    A function to pre-process and predict using the transformer model in batches.\n",
    "    \"\"\"\n",
    "    predictions = None\n",
    "    labels = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_input in batcher(inputs, batch_size):\n",
    "            xp, yp = preprocess(batch_input, tokenizer, device)\n",
    "            out_batch = model(**xp)\n",
    "            predictions = out_batch if predictions is None else torch.cat([predictions, out_batch])\n",
    "            labels = yp if labels is None else torch.cat([labels, yp])\n",
    "\n",
    "        return predictions, labels\n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_clauses(ds_entry: Union[List[str], str], clause_type=['NP', 'ADJP']) -> List[str]:\n",
    "    \"\"\"\n",
    "    Separates the input texts into clauses, and only keeps the ones belonging to the specified types.\n",
    "    If clause_type is None, the texts are split but all the clauses are kept.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_entry\n",
    "        A list of strings that we wish to separate into clauses.\n",
    "    clause_type\n",
    "        A list with the types of clauses to keep. If None, all clauses are kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clause_list\n",
    "        A list with input texts split into clauses.\n",
    "    \"\"\"\n",
    "    s = Sentence(ds_entry)\n",
    "    tagger.predict(s)\n",
    "    clause_list = []\n",
    "    for segment in s.get_labels():\n",
    "        if clause_type is None:\n",
    "            clause_list.append(segment.data_point.text)\n",
    "        elif segment.value in clause_type:\n",
    "            clause_list.append(segment.data_point.text)\n",
    "\n",
    "    return clause_list\n",
    "\n",
    "\n",
    "\n",
    "def batch_activations_fct(model, inputs: List[str], batch_size=64) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A function to extract the activations of input texts in batches.\n",
    "    \"\"\"\n",
    "    activations = None\n",
    "    with torch.no_grad():\n",
    "        for batch_input in batcher(inputs, batch_size):\n",
    "            out_batch = model.features(**batch_input)\n",
    "            activations = out_batch if activations is None else torch.cat([activations, out_batch])\n",
    "        return activations\n",
    "    \n",
    "    \n",
    "\n",
    "def acti_preprocess(activations: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A function to preprocess the activations to work with COCKATIEL\n",
    "    \"\"\"\n",
    "    if len(activations.shape) == 4:\n",
    "        activations = torch.mean(activations, (1, 2))\n",
    "\n",
    "    if isinstance(activations, np.ndarray):\n",
    "        activations = torch.Tensor(activations)\n",
    "    if torch.min(activations) < 0:\n",
    "        raise ValueError(\"Please choose a layer with positive activations.\")\n",
    "\n",
    "    return activations.cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_u_values(sentence, cropped_sentences, model, tokenizer, factorization,\n",
    "                       separate, ignore_words: Optional[List[str]] = None, device='cuda') -> np.ndarray:\n",
    "    if ignore_words is None:\n",
    "        ignore_words = []\n",
    "    with torch.no_grad():\n",
    "        activations = None\n",
    "        for crop_id in range(-1, len(cropped_sentences)):\n",
    "            if crop_id == -1:\n",
    "                perturbated_review = sentence\n",
    "            elif cropped_sentences[crop_id] not in ignore_words:\n",
    "                perturbated_review = separate.join(np.delete(cropped_sentences, crop_id))\n",
    "            else:\n",
    "                continue\n",
    "            tokenized_perturbated_review = tokenizer(perturbated_review, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "            activation = model.features(**tokenized_perturbated_review)\n",
    "            activations = activation if activations is None else torch.cat([activations, activation])\n",
    "\n",
    "        activations = acti_preprocess(activations)\n",
    "        u_values = factorization.transform(activations)\n",
    "        \n",
    "        return u_values\n",
    "    \n",
    "\n",
    "    \n",
    "def calculate_importance(\n",
    "        words: List[str], u_values: np.ndarray, concept_id: int, ignore_words: List[str]\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculates the presence of concepts in the input list of words.\n",
    "    \"\"\"\n",
    "    u_delta = u_values[0, concept_id] - u_values[1:, concept_id]\n",
    "    importances = []\n",
    "    delta_id = 0  # pointer to get current id in importance (as we skip unused word)\n",
    "\n",
    "    for word_id in range(len(words)):\n",
    "        if words[word_id] not in ignore_words:\n",
    "            importances.append(u_delta[delta_id])\n",
    "            delta_id += 1\n",
    "        else:\n",
    "            importances.append(0.0)\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4707b0-4508-4df0-9351-a66081fa5957",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa714f3-ee46-422d-b6a3-44b537f8b028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from typing import List, Optional\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A module for generating plots for the explanations on notebooks.\n",
    "\"\"\"\n",
    "\n",
    "def print_legend(colors, label_to_criterion):\n",
    "    \"\"\"\n",
    "    Prints the legend for the plot in different colors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    colors\n",
    "        A dictionary with the colors for each label.\n",
    "    label_to_criterion\n",
    "        A dictionary with the text to put on each label.\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for label_id in label_to_criterion.keys():\n",
    "        html.append(f'<span style=\"background-color: {colors[label_id]} 0.5); padding: 1px 5px; border: solid 3px ; border-color: {colors[label_id]} 1); #EFEFEF\">{label_to_criterion[label_id]} </span>')\n",
    "    display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(html) + \" </div>\" ))\n",
    "    display(HTML('<br><br>'))\n",
    "\n",
    "\n",
    "def viz_concepts(\n",
    "        text,\n",
    "        explanation,\n",
    "        colors,\n",
    "        ignore_words: Optional[List[str]] = None,\n",
    "        extract_fct: str = \"clause\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the visualization for COCKATIEL's explanations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text\n",
    "        A string with the text we wish to explain.\n",
    "    explanation\n",
    "        An array that corresponds to the output of the occlusion function.\n",
    "    ignore_words\n",
    "        A list of strings to ignore when applying occlusion.\n",
    "    extract_fct\n",
    "        A string indicating whether at which level we wish to explain: \"word\", \"clause\" or \"sentence\".\n",
    "    colors\n",
    "        A dictionary with the colors for each label\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.decode('utf-8')\n",
    "    except:\n",
    "        text = str(text)\n",
    "\n",
    "    if extract_fct == \"clause\":\n",
    "        words = extract_clauses(text, clause_type=None)\n",
    "    else:\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "    l_phi = np.array(explanation)\n",
    "\n",
    "    phi_html = []\n",
    "\n",
    "    p = 0  # pointer to get current color for the words (it does not color words that have no phi)\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in ignore_words:\n",
    "            k = 0\n",
    "            for j in range(len(l_phi)):\n",
    "                if l_phi[k][p] < l_phi[j][p]:\n",
    "                    k = j\n",
    "\n",
    "            if l_phi[k][p] > 0.2:\n",
    "                phi_html.append(f'<span style=\"background-color: {colors[k]} {l_phi[k][p]}); padding: 1px 5px; border: solid 3px ; border-color: {colors[k]} 1); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "            else:\n",
    "                phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "        else:\n",
    "            phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "    display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(phi_html) + \" </div>\" ))\n",
    "    display(HTML('<br><br>'))\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_glob_importances(global_importance_pos, global_importance_neg):\n",
    "\n",
    "    plt.figure(figsize=(22, 5))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    cm = plt.get_cmap('tab20')\n",
    "    plt.bar(range(len(global_importance_pos)), global_importance_pos, color=cm.colors, tick_label=range(len(global_importance_pos)))\n",
    "    plt.title(\"Concepts for Positive Class\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    cm = plt.get_cmap('tab20')\n",
    "    plt.bar(range(len(global_importance_pos)), global_importance_neg, color=cm.colors, tick_label=range(len(global_importance_neg)))\n",
    "    plt.title(\"Concepts for Negative Class\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                     segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                     custom_samples, samples_to_test=None\n",
    "                    ):\n",
    "    \n",
    "    # Find Concept IDs\n",
    "    \n",
    "    m_pos = 2 # Number of concept looked for the positive class \n",
    "    m_neg = 3 # Number of concept looked for the negative class\n",
    "\n",
    "    l_concept_id_pos = np.argsort(global_importance_pos)[::-1][:m_pos]\n",
    "    l_concept_id_neg = np.argsort(global_importance_neg)[::-1][:m_neg]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create dictionnaries for legend with a color for each concept:\n",
    "    # Use m_pos+m_neg concepts, so we have to create a dictionnary \"colors\" with m_pos+m_neg colors. \n",
    "    # In the dictionnary \"label_to_criterion\", add the corresponding concept for each color.\n",
    "\n",
    "    colors = {\n",
    "        0: \"rgba(9, 221, 55, \", #green\n",
    "        1: \"rgba(9, 221, 161, \", #turquoise\n",
    "        #2: \"rgba(9, 175, 221, \", #blue\n",
    "        2: \"rgba(221, 9, 34, \", #red\n",
    "        3: \"rgba(221, 9, 140,\", #pink\n",
    "        4: \"rgba(221, 90, 9, \", #orange\n",
    "    }\n",
    "\n",
    "    label_to_criterion = {\n",
    "        0: \"Positive label: concept1\",\n",
    "        1: \"Positive label: concept2\",\n",
    "        #2: \"Positive label: concept3\",\n",
    "        2: \"Negative label: concept1\",\n",
    "        3: \"Negative label: concept2\",\n",
    "        4: \"Negative label: concept3\",\n",
    "    }\n",
    "\n",
    "\n",
    "    #sanity check:\n",
    "    if len(label_to_criterion.keys()) != len(colors.keys()) or len(label_to_criterion.keys()) != (m_pos + m_neg):\n",
    "        print(\"Error: check that you have the correct number of colors and labels in your dictionaries to cover the number of concepts being looked at\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    print_legend(colors, label_to_criterion)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    # Visualize explanations of custom samples\n",
    "    \n",
    "    if custom_samples is True:\n",
    "    \n",
    "        l_mip_sentences_neg = np.array(samples_to_test[0])\n",
    "        l_mip_sentences_pos = np.array(samples_to_test[1])\n",
    "        \n",
    "        n = len(l_mip_sentences_neg)    # Number of reviews for each labels and each concept\n",
    "        \n",
    "        \n",
    "        for sentence in l_mip_sentences_pos:\n",
    "            \n",
    "            phi = occlusion_concepts(sentence, model, tokenizer, [factorization_pos, factorization_neg], [l_concept_id_pos, l_concept_id_neg], ignore_words = [], two_labels = True, device = device)\n",
    "            phi /= np.max(np.abs(phi)) + 1e-5\n",
    "            viz_concepts(sentence, phi, colors, ignore_words = [])\n",
    "            \n",
    "        for sentence in l_mip_sentences_neg:\n",
    "            \n",
    "            phi = occlusion_concepts(sentence,  model, tokenizer, [factorization_pos, factorization_neg], [l_concept_id_pos, l_concept_id_neg], ignore_words = [], two_labels = True, device = device)\n",
    "            phi /= np.max(np.abs(phi)) + 1e-5\n",
    "            viz_concepts(sentence, phi, colors, ignore_words = [])\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Visualize explanations of IMDB reviews\n",
    "    \n",
    "    elif custom_samples is False:\n",
    "        \n",
    "        # Find most important sentences that are associated with most important concepts\n",
    "        \n",
    "        n = 10\n",
    "        \n",
    "        l_mip_sentences_pos, l_mip_sentences_neg = [], []\n",
    "\n",
    "        # Add sentence of positive label\n",
    "        for concept_id in l_concept_id_pos:\n",
    "\n",
    "            mip_sentences_ids = np.argsort(u_segments_pos[:, concept_id])[::-1][:n]\n",
    "            mip_sentences = np.array(segments_pos)[mip_sentences_ids]\n",
    "            l_mip_sentences_pos.append(mip_sentences)\n",
    "\n",
    "        # Add sentence of negative label\n",
    "        for concept_id in l_concept_id_neg:\n",
    "\n",
    "            mip_sentences_ids = np.argsort(u_segments_neg[:, concept_id])[::-1][:n]\n",
    "            mip_sentences = np.array(segments_neg)[mip_sentences_ids]\n",
    "            l_mip_sentences_neg.append(mip_sentences)\n",
    "\n",
    "        l_mip_sentences_pos = np.array(l_mip_sentences_pos)\n",
    "        l_mip_sentences_pos = l_mip_sentences_pos.flatten()\n",
    "        l_mip_sentences_neg = np.array(l_mip_sentences_neg)\n",
    "        l_mip_sentences_neg = l_mip_sentences_neg.flatten()\n",
    "    \n",
    "\n",
    "    \n",
    "        print(\"positive predicted reviews:\")\n",
    "        print(\"\\n\")\n",
    "        i = 0\n",
    "        for sentence in l_mip_sentences_pos:\n",
    "            if i%n == 0 :\n",
    "                print(\"\\n\")\n",
    "                print(str(n) + \" most important reviews for positive predicted label - concept\" + str(i//n + 1) +\":\")\n",
    "                print(\"\\n\")\n",
    "            phi = occlusion_concepts(sentence, model, tokenizer, [factorization_pos, factorization_neg], [l_concept_id_pos, l_concept_id_neg], ignore_words = [], two_labels = True, device = device)\n",
    "            phi /= np.max(np.abs(phi)) + 1e-5\n",
    "            viz_concepts(sentence, phi, colors, ignore_words = [])\n",
    "            i += 1\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"negative predicted reviews:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        i = 0\n",
    "        for sentence in l_mip_sentences_neg:\n",
    "\n",
    "            if i%n == 0 :\n",
    "                print(\"\\n\")\n",
    "                print(str(n) + \" most important reviews for negative predicted label - concept\" + str(i//n + 1) +\":\")\n",
    "                print(\"\\n\")\n",
    "            phi = occlusion_concepts(sentence,  model, tokenizer, [factorization_pos, factorization_neg], [l_concept_id_pos, l_concept_id_neg], ignore_words = [], two_labels = True, device = device)\n",
    "            phi /= np.max(np.abs(phi)) + 1e-5\n",
    "            viz_concepts(sentence, phi, colors, ignore_words = [])\n",
    "            i +=1\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46954b6c-555f-4bfd-b2bb-38ce49e8d47e",
   "metadata": {},
   "source": [
    "## Personal Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd541a-64fa-4b4f-9e70-a53afbe3be04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeba3a8-661b-459a-9798-82cbee6ed418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "parent_path = \"../Concept-based-Explanation-for-NLP/data/for_roberta/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c34ae6-0011-4638-bdac-be9c8d4188f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616f891-4c99-43db-bfa6-1f637f372a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move the current path to the repo\n",
    "%cd \"../cockatiel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a556b8d-e584-4964-90fc-d477a4d31881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable automatical module reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ec081-ea8a-4189-959f-68e5b4f566d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up GPU if it is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Selected component:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102358e-2bf6-4173-af7e-b79cf5977887",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### IMDB Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb65c11-05c7-4ff1-a2bc-caa2fd2e596a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3006d-f0ad-4b5b-a3ba-867d7674e011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_imbdb_clean = pd.read_csv(\"../Concept-based-Explanation-for-NLP/data/IMDB_Dataset_clean.csv\", index_col=0)\n",
    "df_imbdb_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eecf8d-9f15-4af8-be3e-dd2a3b517dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_imbdb_clean['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ac3bf-2a6c-4d4c-93de-d09e3423406e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract the word counts of each sentence \n",
    "lengths = [len(string.split()) for string in df_imbdb_clean['review']]\n",
    "\n",
    "print(\"Max number of words in a sentence:\", max(lengths))\n",
    "print(\"Min number of words in a sentence:\", min(lengths))\n",
    "print(\"Avg number of words in a sentence:\", round(sum(lengths) / len(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d88153-8a02-4d1f-a76a-f11332bd84de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a histogram to group the data points\n",
    "hist, bins = np.histogram(lengths, bins=150)\n",
    "\n",
    "# Create the histogram plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.hist(lengths, bins=bins, edgecolor='k')\n",
    "plt.xlabel('Number of Words in a Sentence')\n",
    "plt.ylabel('Number of Sentences')\n",
    "plt.title('Distribution of Word Counts in Sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45badbc0-5d1b-4f14-bb8f-e3bfa3977c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to numpy array\n",
    "data_np = np.array(df_imbdb_clean)\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37f49b-5585-432d-825c-57c462a2e7f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Common Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f59f4-c532-4906-bf18-e00bff13ee64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###### the killings are stupid and / or unimaginative bulamadım\n",
    "\n",
    "# substring = \"unimaginative\"\n",
    "\n",
    "# for index, string in enumerate(segments_neg):\n",
    "#     if substring in string:\n",
    "#         print(f\"Substring '{substring}' found in string at index {index}: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564235ab-c7fe-4ac0-bb67-1aad62fb8436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indices_neg = [35253, 29729, 35285, 14800, 14720, 31318, 15747]\n",
    "# reviews_neg = np.array(segments_neg)[indices_neg]\n",
    "# print(reviews_neg)\n",
    "\n",
    "# indices_pos = [10108, 15711, 24157, 4142, 8211, 16541, 31384, 39364]\n",
    "# reviews_pos = np.array(segments_pos)[indices_pos]\n",
    "# print(reviews_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2730d-0b13-4e63-9b91-d54cc6b376d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews_neg = [\n",
    "    \"I couldn't even follow the plot, but suffice it to say, this is the absolute worst movie I have ever seen in my life.\",\n",
    "    \"This is by far one of the worst films I've ever paid good money to see.\",\n",
    "    'Words cannot begin to describe how blandly terrible this movie is.',\n",
    "    \"It's pointless.\",\n",
    "    'Plot is unimportant.',\n",
    "    'About the only good thing about this is the setting.',\n",
    "    \"It's too bad because this film had great production values and a good cast, but isn't the idea of turning a book into a movie (TV or film) to get the people who read the book to be part of the audience.\"\n",
    "]\n",
    "\n",
    "reviews_pos = [\n",
    "    'One of the best movies ever, hands down.',\n",
    "    'It is simply one of my all-time favorite films.',\n",
    "    'Very inspiring and encouraging to all ages.',\n",
    "    'The ballroom scenes were very nice, the dancing and the outfits looked beautiful.',\n",
    "    \"Billy Bitzer's camera work is quite good.\",\n",
    "    'The humor and sadness are subtly blended.',\n",
    "    'Idrissa Oudraogo ( Burkina Faso ): from one of the poorest country in the world, a tender and funny story about five boys who want to capture Osama Bin Laden.',\n",
    "    'Especially fun is the performance by Cronenberg as the truly evil human doctor who is bent on destroying the Nightbreed.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db7d4a-138b-4e4e-b16c-cd823b1f8416",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e7e68-06e3-4552-a24c-29282115753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and its tokenizer\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "# from cockatiel import CustomRobertaForSequenceClassification, batch_predict, batcher, tokenize\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"wrmurray/roberta-base-finetuned-imdb\")\n",
    "model = CustomRobertaForSequenceClassification.from_pretrained(\"wrmurray/roberta-base-finetuned-imdb\").to(device)\n",
    "\n",
    "# Put model into evaluation mode\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# View model summary\n",
    "\n",
    "import torchsummary as ts\n",
    "\n",
    "print(ts.summary(model).total_params)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on some samples\n",
    "y_pred, labels = batch_predict(model.forward, tokenizer, data_np[:100], batch_size, device)\n",
    "\n",
    "# Compute the activations on which to apply the NMF\n",
    "features, labels = batch_predict(model.features, tokenizer, data_np[:100], batch_size, device)\n",
    "\n",
    "# Go from these activations to the final prediction\n",
    "y_pred_bis = model.end_model(features)\n",
    "\n",
    "print(\"\\nAccuracy for classic model        :\", torch.mean((torch.argmax(y_pred, -1) == labels.to(device)).float()))\n",
    "print(\"Accuracy for model in 'two parts' :\", torch.mean((torch.argmax(y_pred_bis, -1) == labels.to(device)).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709693f-9874-4296-94d3-1e589c7f9ec0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train COCKATIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027084ec-22b1-4983-b97d-a1125f60b419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Veri kümesindeki ilk 20K'yı tahminletiyor\n",
    "\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_batch in batcher(data_np[:20000], batch_size=batch_size):\n",
    "        \n",
    "        r = np.array(list(map(lambda z: z[0], input_batch)))\n",
    "\n",
    "        tokenized_batch = tokenize(r, tokenizer, device)\n",
    "        \n",
    "        preds = model(**tokenized_batch)\n",
    "        \n",
    "        positive_reviews.extend(list(input_batch[np.where(np.argmax(preds.cpu().numpy(), axis=1) == 1)[0]]))\n",
    "        negative_reviews.extend(list(input_batch[np.where(np.argmax(preds.cpu().numpy(), axis=1) == 0)[0]]))\n",
    "\n",
    "raw_dataset_pos = list(map(lambda z: z[0], positive_reviews))\n",
    "raw_dataset_neg = list(map(lambda z: z[0], negative_reviews))\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(raw_dataset_pos, open(parent_path + \"raw_dataset_pos.pkl\", \"wb\"))\n",
    "pickle.dump(raw_dataset_neg, open(parent_path + \"raw_dataset_neg.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# Extract the excerpts as a dataset\n",
    "\n",
    "sentence_separators = ['.', '...', '?', '!', '..']\n",
    "\n",
    "def excerpt_fct(raw_dataset):\n",
    "    \n",
    "    excerpt_dataset = []\n",
    "    \n",
    "    for review in raw_dataset[:100000]:\n",
    "        review = str(review)\n",
    "        \n",
    "        for sep in sentence_separators:\n",
    "            review = review.replace(sep, '.')\n",
    "  \n",
    "        cropped_review = review.split('.')\n",
    "    \n",
    "        for crop in cropped_review:\n",
    "            \n",
    "            if len(crop):\n",
    "                crop = crop + '.'\n",
    "                \n",
    "                while crop[0] == ' ':\n",
    "                    crop = crop[1:]\n",
    "      \n",
    "                if crop[0].isupper():  \n",
    "                    excerpt_dataset.append(crop)\n",
    "                \n",
    "    return excerpt_dataset\n",
    "  \n",
    "excerpt_dataset_pos = excerpt_fct(raw_dataset_pos)\n",
    "excerpt_dataset_neg = excerpt_fct(raw_dataset_neg)\n",
    "\n",
    "print(len(excerpt_dataset_pos), ' positives excerpts created.')\n",
    "print(len(excerpt_dataset_neg), ' negatives excerpts created.')\n",
    "\n",
    "pickle.dump(excerpt_dataset_pos, open(parent_path + \"excerpt_dataset_pos.pkl\", \"wb\"))\n",
    "pickle.dump(excerpt_dataset_neg, open(parent_path + \"excerpt_dataset_neg.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# Train COCKATIEL\n",
    "\n",
    "len_data = 50000\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    cockatiel_explainer_pos = COCKATIEL(model, tokenizer, components=20, \n",
    "                                        batch_size=batch_size, device=device)\n",
    "    \n",
    "    # DEĞİŞİKLİK -> batch size was 64 tü buralarda, 32 kullandım \n",
    "    segments_pos, u_segments_pos, factorization_pos, global_importance_pos = cockatiel_explainer_pos.extract_concepts(excerpt_dataset_pos[:len_data], \n",
    "                                                                                                                      raw_dataset_pos[:(len_data//10)], \n",
    "                                                                                                                      1, limit_sobol=1_000)\n",
    "    \n",
    "pickle.dump(segments_pos, open(parent_path + \"segments_pos.pkl\", \"wb\"))\n",
    "pickle.dump(u_segments_pos, open(parent_path + \"u_segments_pos.pkl\", \"wb\"))\n",
    "pickle.dump(factorization_pos, open(parent_path + \"factorization_pos.pkl\", \"wb\"))\n",
    "pickle.dump(global_importance_pos, open(parent_path + \"global_importance_pos.pkl\", \"wb\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    cockatiel_explainer_neg = COCKATIEL(model, tokenizer, components=20, \n",
    "                                        batch_size=batch_size, device=device)\n",
    "    \n",
    "    segments_neg, u_segments_neg, factorization_neg, global_importance_neg = cockatiel_explainer_neg.extract_concepts(excerpt_dataset_neg[:len_data], \n",
    "                                                                                            raw_dataset_neg[:(len_data//10)], \n",
    "                                                                                            0, limit_sobol=1_000)\n",
    "    \n",
    "pickle.dump(segments_neg, open(parent_path + \"segments_neg.pkl\", \"wb\"))\n",
    "pickle.dump(u_segments_neg, open(parent_path + \"u_segments_neg.pkl\", \"wb\"))\n",
    "pickle.dump(factorization_neg, open(parent_path + \"factorization_neg.pkl\", \"wb\"))\n",
    "pickle.dump(global_importance_neg, open(parent_path + \"global_importance_neg.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585a861-4883-4566-8942-6a4b9a32a2b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2395d6c-8976-4246-aaf0-0faffd3bc722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load raw dataset and excerpts\n",
    "\n",
    "raw_dataset_pos = pickle.load(open(parent_path + \"raw_dataset_pos.pkl\", \"rb\"))\n",
    "raw_dataset_neg = pickle.load(open(parent_path + \"raw_dataset_neg.pkl\", \"rb\"))\n",
    "excerpt_dataset_pos = pickle.load(open(parent_path + \"excerpt_dataset_pos.pkl\", \"rb\"))\n",
    "excerpt_dataset_neg = pickle.load(open(parent_path + \"excerpt_dataset_neg.pkl\", \"rb\"))\n",
    "\n",
    "# Load sentences and global importances for positive class\n",
    "\n",
    "segments_pos = pickle.load(open(parent_path + \"segments_pos.pkl\",'rb'))\n",
    "u_segments_pos = pickle.load(open(parent_path + \"u_segments_pos.pkl\",'rb'))\n",
    "factorization_pos = pickle.load(open(parent_path + \"factorization_pos.pkl\",'rb'))\n",
    "global_importance_pos = pickle.load(open(parent_path + \"global_importance_pos.pkl\",'rb'))\n",
    "\n",
    "# Load sentences and global importances for negative class\n",
    "\n",
    "segments_neg = pickle.load(open(parent_path + \"segments_neg.pkl\",'rb'))\n",
    "u_segments_neg = pickle.load(open(parent_path + \"u_segments_neg.pkl\",'rb'))\n",
    "factorization_neg = pickle.load(open(parent_path + \"factorization_neg.pkl\",'rb'))\n",
    "global_importance_neg = pickle.load(open(parent_path + \"global_importance_neg.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f0cee-1ffc-43ba-8a67-bb2ad4a77ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_glob_importances(global_importance_pos, global_importance_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7a96-0cf4-4c74-8a3e-59d49de8bfe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                 segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                 custom_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c279c-0976-4ea0-aa4b-68236732cda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                 segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                 custom_samples=True, samples_to_test=[reviews_neg, reviews_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f554b-b1a9-41f4-9115-82ae8ea1847d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### AlBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd386c-f7da-4a45-957f-9de359927eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model and its tokenizer\n",
    "\n",
    "from transformers import AlbertTokenizerFast\n",
    "from cockatiel import CustomAlbertForSequenceClassification, batch_predict, batcher, tokenize\n",
    "\n",
    "tokenizer = AlbertTokenizerFast.from_pretrained(\"Ibrahim-Alam/finetuning-albert-base-v2-on-imdb\")\n",
    "model = CustomAlbertForSequenceClassification.from_pretrained(\"Ibrahim-Alam/finetuning-albert-base-v2-on-imdb\").to(device)\n",
    "\n",
    "# Put model into evaluation mode\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# View model summary\n",
    "\n",
    "import torchsummary as ts\n",
    "\n",
    "print(ts.summary(model).total_params)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on some samples\n",
    "y_pred, labels = batch_predict(model.forward, tokenizer, data_np[:100], batch_size, device)\n",
    "\n",
    "# Compute the activations on which to apply the NMF\n",
    "features, labels = batch_predict(model.features, tokenizer, data_np[:100], batch_size, device)\n",
    "\n",
    "# Go from these activations to the final prediction\n",
    "y_pred_bis = model.end_model(features)\n",
    "\n",
    "print(\"\\nAccuracy for classic model        :\", torch.mean((torch.argmax(y_pred, -1) == labels.to(device)).float()))\n",
    "print(\"Accuracy for model in 'two parts' :\", torch.mean((torch.argmax(y_pred_bis, -1) == labels.to(device)).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8e6b5-0c14-4e3a-b50d-c4584126355c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Train COCKATIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146754a6-0d9a-4f0c-8215-b94eb5f8fe42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Veri kümesindeki ilk 20K'yı tahminletiyor\n",
    "\n",
    "# positive_reviews = []\n",
    "# negative_reviews = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for input_batch in batcher(data_np[:20000], batch_size=batch_size):\n",
    "        \n",
    "#         r = np.array(list(map(lambda z: z[0], input_batch)))\n",
    "\n",
    "#         tokenized_batch = tokenize(r, tokenizer, device)\n",
    "        \n",
    "#         preds = model(**tokenized_batch)\n",
    "        \n",
    "#         positive_reviews.extend(list(input_batch[np.where(np.argmax(preds.cpu().numpy(), axis=1) == 1)[0]]))\n",
    "#         negative_reviews.extend(list(input_batch[np.where(np.argmax(preds.cpu().numpy(), axis=1) == 0)[0]]))\n",
    "\n",
    "# raw_dataset_pos = list(map(lambda z: z[0], positive_reviews))\n",
    "# raw_dataset_neg = list(map(lambda z: z[0], negative_reviews))\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# pickle.dump(raw_dataset_pos, open(parent_path + \"raw_dataset_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(raw_dataset_neg, open(parent_path + \"raw_dataset_neg.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the excerpts as a dataset\n",
    "\n",
    "# sentence_separators = ['.', '...', '?', '!', '..']\n",
    "\n",
    "# def excerpt_fct(raw_dataset):\n",
    "    \n",
    "#     excerpt_dataset = []\n",
    "    \n",
    "#     for review in raw_dataset[:100000]:\n",
    "#         review = str(review)\n",
    "        \n",
    "#         for sep in sentence_separators:\n",
    "#             review = review.replace(sep, '.')\n",
    "  \n",
    "#         cropped_review = review.split('.')\n",
    "    \n",
    "#         for crop in cropped_review:\n",
    "            \n",
    "#             if len(crop):\n",
    "#                 crop = crop + '.'\n",
    "                \n",
    "#                 while crop[0] == ' ':\n",
    "#                     crop = crop[1:]\n",
    "      \n",
    "#                 if crop[0].isupper():  \n",
    "#                     excerpt_dataset.append(crop)\n",
    "                \n",
    "#     return excerpt_dataset\n",
    "  \n",
    "# excerpt_dataset_pos = excerpt_fct(raw_dataset_pos)\n",
    "# excerpt_dataset_neg = excerpt_fct(raw_dataset_neg)\n",
    "\n",
    "# print(len(excerpt_dataset_pos), ' positives excerpts created.')\n",
    "# print(len(excerpt_dataset_neg), ' negatives excerpts created.')\n",
    "\n",
    "# pickle.dump(excerpt_dataset_pos, open(parent_path + \"excerpt_dataset_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(excerpt_dataset_neg, open(parent_path + \"excerpt_dataset_neg.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Train COCKATIEL\n",
    "\n",
    "# len_data = 50000\n",
    "\n",
    "# with torch.no_grad():\n",
    "    \n",
    "#     cockatiel_explainer_pos = COCKATIEL(model, tokenizer, components=20, \n",
    "#                                         batch_size=batch_size, device=device)\n",
    "#     # batch size was 64 \n",
    "#     segments_pos, u_segments_pos, factorization_pos, global_importance_pos = cockatiel_explainer_pos.extract_concepts(excerpt_dataset_pos[:len_data], \n",
    "#                                                                                                                       raw_dataset_pos[:(len_data//10)], \n",
    "#                                                                                                                       1, limit_sobol=1_000)\n",
    "    \n",
    "# pickle.dump(segments_pos, open(parent_path + \"segments_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(u_segments_pos, open(parent_path + \"u_segments_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(factorization_pos, open(parent_path + \"factorization_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(global_importance_pos, open(parent_path + \"global_importance_pos.pkl\", \"wb\"))\n",
    "\n",
    "# with torch.no_grad():\n",
    "    \n",
    "#     cockatiel_explainer_neg = COCKATIEL(model, tokenizer, components=20, \n",
    "#                                         batch_size=batch_size, device=device)\n",
    "    \n",
    "#     segments_neg, u_segments_neg, factorization_neg, global_importance_neg = cockatiel_explainer_neg.extract_concepts(excerpt_dataset_neg[:len_data], \n",
    "#                                                                                             raw_dataset_neg[:(len_data//10)], \n",
    "#                                                                                             0, limit_sobol=1_000)\n",
    "    \n",
    "# pickle.dump(segments_neg, open(parent_path + \"segments_neg.pkl\", \"wb\"))\n",
    "# pickle.dump(u_segments_neg, open(parent_path + \"u_segments_neg.pkl\", \"wb\"))\n",
    "# pickle.dump(factorization_neg, open(parent_path + \"factorization_neg.pkl\", \"wb\"))\n",
    "# pickle.dump(global_importance_neg, open(parent_path + \"global_importance_neg.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3c724-c751-4fda-8d80-e212ac460c9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fc7cc-944a-49b6-a216-9367cbf8d08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load raw dataset and excerpts\n",
    "\n",
    "raw_dataset_pos = pickle.load(open(parent_path + \"raw_dataset_pos.pkl\", \"rb\"))\n",
    "raw_dataset_neg = pickle.load(open(parent_path + \"raw_dataset_neg.pkl\", \"rb\"))\n",
    "excerpt_dataset_pos = pickle.load(open(parent_path + \"excerpt_dataset_pos.pkl\", \"rb\"))\n",
    "excerpt_dataset_neg = pickle.load(open(parent_path + \"excerpt_dataset_neg.pkl\", \"rb\"))\n",
    "\n",
    "# Load sentences and global importances for positive class\n",
    "\n",
    "segments_pos = pickle.load(open(parent_path + \"segments_pos.pkl\",'rb'))\n",
    "u_segments_pos = pickle.load(open(parent_path + \"u_segments_pos.pkl\",'rb'))\n",
    "factorization_pos = pickle.load(open(parent_path + \"factorization_pos.pkl\",'rb'))\n",
    "global_importance_pos = pickle.load(open(parent_path + \"global_importance_pos.pkl\",'rb'))\n",
    "\n",
    "# Load sentences and global importances for negative class\n",
    "\n",
    "segments_neg = pickle.load(open(parent_path + \"segments_neg.pkl\",'rb'))\n",
    "u_segments_neg = pickle.load(open(parent_path + \"u_segments_neg.pkl\",'rb'))\n",
    "factorization_neg = pickle.load(open(parent_path + \"factorization_neg.pkl\",'rb'))\n",
    "global_importance_neg = pickle.load(open(parent_path + \"global_importance_neg.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8714e35-d14f-4423-b7e8-5b4f3835c69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_glob_importances(global_importance_pos, global_importance_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4b96f-e355-4b38-990a-66a169573c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                 segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                 custom_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c8852-304d-4055-a1ac-f7bf68b8e66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                 segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                 custom_samples=True, samples_to_test=[reviews_neg, reviews_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c504804-4ee7-4a25-b7d3-303a1cb12450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdf926-7374-4cc8-bd2f-6fef0b8c6388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model and its tokenizer\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from cockatiel import CustomDistilbertForSequenceClassification, batch_predict, batcher, tokenize\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"federicopascual/finetuned-sentiment-analysis-model\")\n",
    "model = CustomDistilbertForSequenceClassification.from_pretrained(\"federicopascual/finetuned-sentiment-analysis-model\").to(device)\n",
    "\n",
    "# Put model into evaluation mode\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# View model summary\n",
    "\n",
    "import torchsummary as ts\n",
    "\n",
    "print(ts.summary(model).total_params)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on some samples\n",
    "y_pred, labels = batch_predict(model.forward, tokenizer, data_np[:100], batch_size, device)\n",
    "\n",
    "# Compute the activations on which to apply the NMF\n",
    "features, labels = batch_predict(model.features, tokenizer, data_np[:100], batch_size, device)\n",
    "\n",
    "# Go from these activations to the final prediction\n",
    "y_pred_bis = model.end_model(features)\n",
    "\n",
    "print(\"\\nAccuracy for classic model        :\", torch.mean((torch.argmax(y_pred, -1) == labels.to(device)).float()))\n",
    "print(\"Accuracy for model in 'two parts' :\", torch.mean((torch.argmax(y_pred_bis, -1) == labels.to(device)).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a51dae-8e2e-4f80-8af3-3acc7d0b7c98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Train COCKATIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c97dd-3dcd-4300-903d-2539bb1ffe3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Veri kümesindeki ilk 20K'yı tahminletiyor\n",
    "\n",
    "# positive_reviews = []\n",
    "# negative_reviews = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for input_batch in batcher(data_np[:20000], batch_size=batch_size):\n",
    "        \n",
    "#         r = np.array(list(map(lambda z: z[0], input_batch)))\n",
    "\n",
    "#         tokenized_batch = tokenize(r, tokenizer, device)\n",
    "        \n",
    "#         preds = model(**tokenized_batch)\n",
    "        \n",
    "#         positive_reviews.extend(list(input_batch[np.where(np.argmax(preds.cpu().numpy(), axis=1) == 1)[0]]))\n",
    "#         negative_reviews.extend(list(input_batch[np.where(np.argmax(preds.cpu().numpy(), axis=1) == 0)[0]]))\n",
    "\n",
    "# raw_dataset_pos = list(map(lambda z: z[0], positive_reviews))\n",
    "# raw_dataset_neg = list(map(lambda z: z[0], negative_reviews))\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# pickle.dump(raw_dataset_pos, open(parent_path + \"raw_dataset_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(raw_dataset_neg, open(parent_path + \"raw_dataset_neg.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Extract the excerpts as a dataset\n",
    "\n",
    "# sentence_separators = ['.', '...', '?', '!', '..']\n",
    "\n",
    "# def excerpt_fct(raw_dataset):\n",
    "    \n",
    "#     excerpt_dataset = []\n",
    "    \n",
    "#     for review in raw_dataset[:100000]:\n",
    "#         review = str(review)\n",
    "        \n",
    "#         for sep in sentence_separators:\n",
    "#             review = review.replace(sep, '.')\n",
    "  \n",
    "#         cropped_review = review.split('.')\n",
    "    \n",
    "#         for crop in cropped_review:\n",
    "            \n",
    "#             if len(crop):\n",
    "#                 crop = crop + '.'\n",
    "                \n",
    "#                 while crop[0] == ' ':\n",
    "#                     crop = crop[1:]\n",
    "      \n",
    "#                 if crop[0].isupper():  \n",
    "#                     excerpt_dataset.append(crop)\n",
    "                \n",
    "#     return excerpt_dataset\n",
    "  \n",
    "# excerpt_dataset_pos = excerpt_fct(raw_dataset_pos)\n",
    "# excerpt_dataset_neg = excerpt_fct(raw_dataset_neg)\n",
    "\n",
    "# print(len(excerpt_dataset_pos), ' positives excerpts created.')\n",
    "# print(len(excerpt_dataset_neg), ' negatives excerpts created.')\n",
    "\n",
    "# pickle.dump(excerpt_dataset_pos, open(parent_path + \"excerpt_dataset_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(excerpt_dataset_neg, open(parent_path + \"excerpt_dataset_neg.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Train COCKATIEL\n",
    "\n",
    "# len_data = 50000\n",
    "\n",
    "# with torch.no_grad():\n",
    "    \n",
    "#     cockatiel_explainer_pos = COCKATIEL(model, tokenizer, components=20, \n",
    "#                                         batch_size=batch_size, device=device)\n",
    "#     # batch size was 64 \n",
    "#     segments_pos, u_segments_pos, factorization_pos, global_importance_pos = cockatiel_explainer_pos.extract_concepts(excerpt_dataset_pos[:len_data], \n",
    "#                                                                                                                       raw_dataset_pos[:(len_data//10)], \n",
    "#                                                                                                                       1, limit_sobol=1_000)\n",
    "    \n",
    "# pickle.dump(segments_pos, open(parent_path + \"segments_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(u_segments_pos, open(parent_path + \"u_segments_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(factorization_pos, open(parent_path + \"factorization_pos.pkl\", \"wb\"))\n",
    "# pickle.dump(global_importance_pos, open(parent_path + \"global_importance_pos.pkl\", \"wb\"))\n",
    "\n",
    "# with torch.no_grad():\n",
    "    \n",
    "#     cockatiel_explainer_neg = COCKATIEL(model, tokenizer, components=20, \n",
    "#                                         batch_size=batch_size, device=device)\n",
    "    \n",
    "#     segments_neg, u_segments_neg, factorization_neg, global_importance_neg = cockatiel_explainer_neg.extract_concepts(excerpt_dataset_neg[:len_data], \n",
    "#                                                                                             raw_dataset_neg[:(len_data//10)], \n",
    "#                                                                                             0, limit_sobol=1_000)\n",
    "    \n",
    "# pickle.dump(segments_neg, open(parent_path + \"segments_neg.pkl\", \"wb\"))\n",
    "# pickle.dump(u_segments_neg, open(parent_path + \"u_segments_neg.pkl\", \"wb\"))\n",
    "# pickle.dump(factorization_neg, open(parent_path + \"factorization_neg.pkl\", \"wb\"))\n",
    "# pickle.dump(global_importance_neg, open(parent_path + \"global_importance_neg.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad0509-b252-4935-8bba-9a72338e2c95",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe694b9-3086-42dc-bad2-0f84b189bf5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load raw dataset and excerpts\n",
    "\n",
    "raw_dataset_pos = pickle.load(open(parent_path + \"raw_dataset_pos.pkl\", \"rb\"))\n",
    "raw_dataset_neg = pickle.load(open(parent_path + \"raw_dataset_neg.pkl\", \"rb\"))\n",
    "excerpt_dataset_pos = pickle.load(open(parent_path + \"excerpt_dataset_pos.pkl\", \"rb\"))\n",
    "excerpt_dataset_neg = pickle.load(open(parent_path + \"excerpt_dataset_neg.pkl\", \"rb\"))\n",
    "\n",
    "# Load sentences and global importances for positive class\n",
    "\n",
    "segments_pos = pickle.load(open(parent_path + \"segments_pos.pkl\",'rb'))\n",
    "u_segments_pos = pickle.load(open(parent_path + \"u_segments_pos.pkl\",'rb'))\n",
    "factorization_pos = pickle.load(open(parent_path + \"factorization_pos.pkl\",'rb'))\n",
    "global_importance_pos = pickle.load(open(parent_path + \"global_importance_pos.pkl\",'rb'))\n",
    "\n",
    "# Load sentences and global importances for negative class\n",
    "\n",
    "segments_neg = pickle.load(open(parent_path + \"segments_neg.pkl\",'rb'))\n",
    "u_segments_neg = pickle.load(open(parent_path + \"u_segments_neg.pkl\",'rb'))\n",
    "factorization_neg = pickle.load(open(parent_path + \"factorization_neg.pkl\",'rb'))\n",
    "global_importance_neg = pickle.load(open(parent_path + \"global_importance_neg.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab5049-0298-478a-9949-f2fa450df778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_glob_importances(global_importance_pos, global_importance_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ebb594-4bf4-432b-be85-c3ecb6293456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                 segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                 custom_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94823b4c-482e-4a87-9d22-aa21996c1a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_reviews_exp(segments_pos, u_segments_pos, factorization_pos, global_importance_pos,\n",
    "                 segments_neg, u_segments_neg, factorization_neg, global_importance_neg,\n",
    "                 custom_samples=True, samples_to_test=[reviews_neg, reviews_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c0843-7233-44b0-a8c2-2c87506f54e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Workshop Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68859c0a-08ab-4425-a598-3572ed9f7418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b16479d0-17f5-410d-9918-cab7b5aad3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe0e2f2c-138c-4f91-a12a-d2368abefb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of NMF(max_iter=1000, n_components=20)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorization_pos.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed37facf-7563-47b8-aabb-9ba1f96d6cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "95f1d840-7900-4712-90b0-d0c3fe8b91a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probabilities = F.softmax(y_pred, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b8fd99c-0552-4694-99cf-c5ef3f1159e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0264, 0.9736],\n",
       "        [0.0180, 0.9820],\n",
       "        [0.0218, 0.9782],\n",
       "        [0.9862, 0.0138],\n",
       "        [0.0215, 0.9785],\n",
       "        [0.0177, 0.9823],\n",
       "        [0.0622, 0.9378],\n",
       "        [0.9886, 0.0114],\n",
       "        [0.9901, 0.0099],\n",
       "        [0.0196, 0.9804],\n",
       "        [0.9596, 0.0404],\n",
       "        [0.5649, 0.4351],\n",
       "        [0.4100, 0.5900],\n",
       "        [0.9541, 0.0459],\n",
       "        [0.0234, 0.9766],\n",
       "        [0.9898, 0.0102],\n",
       "        [0.0595, 0.9405],\n",
       "        [0.9899, 0.0101],\n",
       "        [0.0563, 0.9437],\n",
       "        [0.9892, 0.0108],\n",
       "        [0.0253, 0.9747],\n",
       "        [0.9897, 0.0103],\n",
       "        [0.0178, 0.9822],\n",
       "        [0.9894, 0.0106],\n",
       "        [0.9896, 0.0104],\n",
       "        [0.0703, 0.9297],\n",
       "        [0.0194, 0.9806],\n",
       "        [0.9778, 0.0222],\n",
       "        [0.9862, 0.0138],\n",
       "        [0.0229, 0.9771],\n",
       "        [0.0247, 0.9753],\n",
       "        [0.0185, 0.9815],\n",
       "        [0.9855, 0.0145],\n",
       "        [0.0196, 0.9804],\n",
       "        [0.9891, 0.0109],\n",
       "        [0.9847, 0.0153],\n",
       "        [0.9888, 0.0112],\n",
       "        [0.9877, 0.0123],\n",
       "        [0.0284, 0.9716],\n",
       "        [0.9900, 0.0100],\n",
       "        [0.9893, 0.0107],\n",
       "        [0.0180, 0.9820],\n",
       "        [0.9894, 0.0106],\n",
       "        [0.9878, 0.0122],\n",
       "        [0.0278, 0.9722],\n",
       "        [0.0214, 0.9786],\n",
       "        [0.9863, 0.0137],\n",
       "        [0.9362, 0.0638],\n",
       "        [0.0414, 0.9586],\n",
       "        [0.9836, 0.0164],\n",
       "        [0.0285, 0.9715],\n",
       "        [0.0449, 0.9551],\n",
       "        [0.0308, 0.9692],\n",
       "        [0.0195, 0.9805],\n",
       "        [0.9598, 0.0402],\n",
       "        [0.9871, 0.0129],\n",
       "        [0.9637, 0.0363],\n",
       "        [0.9888, 0.0112],\n",
       "        [0.0241, 0.9759],\n",
       "        [0.0178, 0.9822],\n",
       "        [0.9874, 0.0126],\n",
       "        [0.9839, 0.0161],\n",
       "        [0.0337, 0.9663],\n",
       "        [0.9897, 0.0103],\n",
       "        [0.9886, 0.0114],\n",
       "        [0.0192, 0.9808],\n",
       "        [0.9831, 0.0169],\n",
       "        [0.6621, 0.3379],\n",
       "        [0.9799, 0.0201],\n",
       "        [0.9885, 0.0115],\n",
       "        [0.8380, 0.1620],\n",
       "        [0.9787, 0.0213],\n",
       "        [0.0225, 0.9775],\n",
       "        [0.0247, 0.9753],\n",
       "        [0.9378, 0.0622],\n",
       "        [0.0231, 0.9769],\n",
       "        [0.0363, 0.9637],\n",
       "        [0.9884, 0.0116],\n",
       "        [0.9898, 0.0102],\n",
       "        [0.0153, 0.9847],\n",
       "        [0.0276, 0.9724],\n",
       "        [0.9861, 0.0139],\n",
       "        [0.9881, 0.0119],\n",
       "        [0.9899, 0.0101],\n",
       "        [0.9900, 0.0100],\n",
       "        [0.9896, 0.0104],\n",
       "        [0.9866, 0.0134],\n",
       "        [0.9850, 0.0150],\n",
       "        [0.9883, 0.0117],\n",
       "        [0.9839, 0.0161],\n",
       "        [0.0199, 0.9801],\n",
       "        [0.9881, 0.0119],\n",
       "        [0.0207, 0.9793],\n",
       "        [0.0184, 0.9816],\n",
       "        [0.9559, 0.0441],\n",
       "        [0.0170, 0.9830],\n",
       "        [0.9886, 0.0114],\n",
       "        [0.9897, 0.0103],\n",
       "        [0.9900, 0.0100],\n",
       "        [0.0308, 0.9692]], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7bf85-c1b0-4885-9340-9a5e098453eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your concept count range, e.g., from 1 to N\n",
    "concept_counts = range(1, N+1)\n",
    "\n",
    "# Lists to store fidelity scores for deletion and insertion\n",
    "deletion_fidelity = []\n",
    "insertion_fidelity = []\n",
    "\n",
    "# Model logits scores (for example)\n",
    "average_logits_scores = []\n",
    "\n",
    "for concept_count in concept_counts:\n",
    "    # Obtain concept scores using COCKATIEL for the given concept count\n",
    "    concept_scores = compute_concept_scores(concept_count)\n",
    "\n",
    "    # Compute fidelity scores (deletion and insertion) based on your criteria\n",
    "    deletion_score = compute_deletion_fidelity(concept_scores)\n",
    "    insertion_score = compute_insertion_fidelity(concept_scores)\n",
    "\n",
    "    deletion_fidelity.append(deletion_score)\n",
    "    insertion_fidelity.append(insertion_score)\n",
    "\n",
    "    # Calculate average logits score for your model's predictions\n",
    "    avg_logits = compute_average_logits(concept_count)\n",
    "    average_logits_scores.append(avg_logits)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure()\n",
    "plt.plot(concept_counts, deletion_fidelity, label='Deletion Fidelity', marker='o')\n",
    "plt.plot(concept_counts, insertion_fidelity, label='Insertion Fidelity', marker='o')\n",
    "plt.plot(concept_counts, average_logits_scores, label='Average Logits Score', linestyle='--')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Number of Concepts')\n",
    "plt.ylabel('Scores')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9bcc14-91cb-4a61-9965-f5955456f5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf45076-056d-4f89-b13e-a39360f499b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563d795-88f5-44dd-8440-c86ff316d6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afee32c-b00a-444d-8ee2-87c01e89a223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304339c-1fa6-4358-a01b-e4fee8b1870d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cockatiel",
   "language": "python",
   "name": "cockatiel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
